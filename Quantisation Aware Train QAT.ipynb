{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import numpy as np\n",
    "import copy\n",
    "\n",
    "from sklearn.model_selection import train_test_split"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "\n",
    "data = pd.read_csv(\"c://Users/manpresingh/OneDrive - Microsoft/Personal/PyTorch_and_Advanced NLP/creditcard.csv\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "features = ['V1', 'V2', 'V3', 'V4', 'V5', 'V6', 'V7', 'V8', 'V9', 'V10',\n",
    "       'V11', 'V12', 'V13', 'V14', 'V15', 'V16', 'V17', 'V18', 'V19', 'V20',\n",
    "       'V21', 'V22', 'V23', 'V24', 'V25', 'V26', 'V27', 'V28', 'Amount']\n",
    "target = ['Class']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = data[features]\n",
    "y = data[target]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train, X_test, y_train, y_test = train_test_split(X, y, stratify=y, random_state=100, test_size=.3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train.reset_index(drop=True, inplace=True)\n",
    "X_test.reset_index(drop=True, inplace=True)\n",
    "y_train.reset_index(drop=True, inplace=True)\n",
    "y_test.reset_index(drop=True, inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "X_train_torch = torch.tensor(X_train.values, dtype=torch.float32)\n",
    "X_test_torch = torch.tensor(X_test.values, dtype=torch.float32)\n",
    "\n",
    "y_train_torch = torch.tensor(y_train.values, dtype=torch.float32)\n",
    "y_test_torch = torch.tensor(y_test.values, dtype=torch.float32)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataset = torch.utils.data.TensorDataset(X_train_torch, y_train_torch)\n",
    "# or\n",
    "# This is method2:\n",
    "train_dataset2= list(zip(X_train_torch, y_train_torch))\n",
    "\n",
    "test_dataset = torch.utils.data.TensorDataset(X_test_torch, y_test_torch)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(X_train.index))\n",
    "\n",
    "\n",
    "test_sampler = torch.utils.data.sampler.SubsetRandomSampler(list(X_test.index))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_dataloader = torch.utils.data.DataLoader(train_dataset2, batch_size=128,\n",
    "                                               sampler=train_sampler,\n",
    "                                               num_workers=0)\n",
    "\n",
    "test_dataloader = torch.utils.data.DataLoader(test_dataset, batch_size=128,\n",
    "                                              sampler=test_sampler,\n",
    "                                              num_workers=0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using Basic MixMaxObserver quantisation method"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)  # Output layer for binary classification\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Sigmoid activation for binary classification\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure QAT\n",
    "model.qconfig = torch.quantization.default_qconfig\n",
    "# get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=29, out_features=128, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (fc3): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (fc4): Linear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    (activation_post_process): MinMaxObserver(min_val=inf, max_val=-inf)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "========= Complete TrainLoss = 0.0192\n",
      "========= Complete TestLoss = 0.0192\n",
      "\n",
      "Epoch: 1\n",
      "========= Complete TrainLoss = 0.0221\n",
      "========= Complete TestLoss = 0.0238\n",
      "\n",
      "Epoch: 2\n",
      "========= Complete TrainLoss = 0.0095\n",
      "========= Complete TestLoss = 0.0097\n",
      "\n",
      "Epoch: 3\n",
      "========= Complete TrainLoss = 0.0178\n",
      "========= Complete TestLoss = 0.0189\n",
      "\n",
      "Epoch: 4\n",
      "========= Complete TrainLoss = 0.0185\n",
      "========= Complete TestLoss = 0.0202\n",
      "\n",
      "Epoch: 5\n",
      "========= Complete TrainLoss = 0.0086\n",
      "========= Complete TestLoss = 0.0089\n",
      "\n",
      "Epoch: 6\n",
      "========= Complete TrainLoss = 0.0047\n",
      "========= Complete TestLoss = 0.0055\n",
      "\n",
      "Epoch: 7\n",
      "========= Complete TrainLoss = 0.0041\n",
      "========= Complete TestLoss = 0.0050\n",
      "\n",
      "Epoch: 8\n",
      "========= Complete TrainLoss = 0.0029\n",
      "========= Complete TestLoss = 0.0040\n",
      "\n",
      "Epoch: 9\n",
      "========= Complete TrainLoss = 0.0030\n",
      "========= Complete TestLoss = 0.0042\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "loss_dict={\n",
    "        'train_loss':[],\n",
    "        'test_loss':[]\n",
    "}\n",
    "\n",
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (x_data, y) in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_data)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        with torch.no_grad():\n",
    "                output = model(X_train_torch)\n",
    "                trainloss = criterion(output, y_train_torch)\n",
    "        print(f\"========= Complete TrainLoss = {trainloss.item():.4f}\")\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                val_output = model(X_test_torch)\n",
    "                val_loss = criterion(val_output, y_test_torch)\n",
    "        print(f\"========= Complete TestLoss = {val_loss.item():.4f}\\n\")\n",
    "        loss_dict['test_loss'].append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        # Early Stopping Code        \n",
    "        if val_loss < best_loss:\n",
    "                best_loss=val_loss\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                best_model = model\n",
    "                patience = 10 \n",
    "        else:\n",
    "                patience=-1\n",
    "                if patience==0:\n",
    "                        break;  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int8\n",
    "model.eval()\n",
    "model.cpu()\n",
    "model_quant = torch.ao.quantization.convert(model)\n",
    "\n",
    "# Now you have a quantised binary classification model ready for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): MinMaxObserver(min_val=-113.7433090209961, max_val=25691.16015625)\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=29, out_features=128, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.9271302819252014, max_val=0.863592267036438)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-7253.8505859375, max_val=4703.92578125)\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.6757254004478455, max_val=1.8166415691375732)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-6090.2421875, max_val=14142.7509765625)\n",
       "  )\n",
       "  (fc3): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.6025041937828064, max_val=0.8040540814399719)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-15654.41015625, max_val=32907.72265625)\n",
       "  )\n",
       "  (fc4): Linear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (weight_fake_quant): MinMaxObserver(min_val=-0.20348449051380157, max_val=0.20739072561264038)\n",
       "    (activation_post_process): MinMaxObserver(min_val=-9727.998046875, max_val=37.94682312011719)\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "128"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "2**7"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(203.1811023622047, 0.560295566502463)"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#  (quant): QuantStub(\n",
    "#     (activation_post_process): MinMaxObserver(min_val=-113.7433090209961, max_val=25691.16015625)\n",
    "#   )\n",
    "\n",
    "(25691+113)/(2**7-1), -1*(-113.74)/203"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(107.94488188976378, 82.16767021769337)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# (fc1): Linear(\n",
    "#     in_features=29, out_features=128, bias=True\n",
    "#     (weight_fake_quant): MinMaxObserver(min_val=-0.8365252017974854, max_val=0.930539608001709)\n",
    "#     (activation_post_process): MinMaxObserver(min_val=-8870.5517578125, max_val=4839.1591796875)\n",
    "#   )\n",
    "# in8 means 1 bit for sign and 8 bits Mantissa\n",
    "# hence we use 2**7\n",
    "\n",
    "(4839+8870)/(2**7-1), -1*(-8870)/107.95"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): Quantize(scale=tensor([203.1882]), zero_point=tensor([1]), dtype=torch.quint8)\n",
       "  (fc1): QuantizedLinear(in_features=29, out_features=128, scale=94.15571594238281, zero_point=77, qscheme=torch.per_tensor_affine)\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=64, scale=159.3148956298828, zero_point=38, qscheme=torch.per_tensor_affine)\n",
       "  (fc3): QuantizedLinear(in_features=64, out_features=32, scale=382.3789978027344, zero_point=41, qscheme=torch.per_tensor_affine)\n",
       "  (fc4): QuantizedLinear(in_features=32, out_features=1, scale=76.89720916748047, zero_point=127, qscheme=torch.per_tensor_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.8560, grad_fn=<MaxBackward1>),\n",
       " tensor(-0.9173, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight.max(), model.fc1.weight.min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(118, dtype=torch.int8), tensor(-126, dtype=torch.int8))"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.int_repr(model_quant.fc1.weight()).max(), torch.int_repr(model_quant.fc1.weight()).min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "torch.Size([128, 29])"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.int_repr(model_quant.fc1.weight()).shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ 28,  31,  -3,  ..., -35,  23,  -2],\n",
       "        [-22,  19,  62,  ...,  -7,  10,   1],\n",
       "        [-17,  -8,  98,  ...,  20,  48,   1],\n",
       "        ...,\n",
       "        [-17, -15, -38,  ...,  -8,   2,   9],\n",
       "        [ 20,  27,  12,  ...,  46,  50,  12],\n",
       "        [ -7,  -1,  11,  ...,  25,  14,  -2]], dtype=torch.int8)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## Quantised repre of fc1 weights:\n",
    "torch.int_repr(model_quant.fc1.weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "## basic MinMaxQuantiser is not good and leads a poor quantised model as can be seen below:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9738, -0.2370, -0.2398,  0.4631, -0.6509, -0.7381, -0.3787, -0.1465,\n",
       "         1.1360, -0.1925, -0.6413,  1.0334,  1.0345, -0.2216,  0.5477,  0.1455,\n",
       "        -0.5923, -0.3133, -0.0873, -0.1571, -0.1655, -0.2883,  0.3579,  0.0276,\n",
       "        -0.3998, -0.6285,  0.0404, -0.0276, 12.4900])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[120][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9738, -0.2370, -0.2398,  0.4631, -0.6509, -0.7381, -0.3787, -0.1465,\n",
       "         1.1360, -0.1925, -0.6413,  1.0334,  1.0345, -0.2216,  0.5477,  0.1455,\n",
       "        -0.5923, -0.3133, -0.0873, -0.1571, -0.1655, -0.2883,  0.3579,  0.0276,\n",
       "        -0.3998, -0.6285,  0.0404, -0.0276, 12.4900])"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.quant ; this results in as-is as quant step is not learning properly here\n",
    "model.quant(train_dataloader.dataset[120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0., 0.,\n",
       "        0., 0., 0., 0., 0.], size=(29,), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=203.18821716308594,\n",
       "       zero_point=1)"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.quant\n",
    "model_quant.quant(train_dataloader.dataset[120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[7.5146e-07],\n",
       "        [9.9282e-01]], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][1644:1646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.5000],\n",
       "        [0.5000]])"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Both prob are .5, poor model\n",
    "\n",
    "model_quant.forward(train_dataset.tensors[0:1][0][1644:1646])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1645,   2850,   3260,   3714,   3802,   3854,   3891,   4032,   4445,\n",
       "          4557,   5607,   6681,   7452,   7748,   8020,  10130,  10207,  10752,\n",
       "         11702,  11891,  13150,  13462,  14836,  15513,  16184,  17138,  18179,\n",
       "         19013,  19020,  20284,  20618,  20794,  21526,  21757,  21797,  21833,\n",
       "         22028,  22979,  23133,  23134,  23504,  24057,  24266,  25626,  25792,\n",
       "         26150,  27258,  27507,  27661,  28342,  28760,  29306,  30713,  31760,\n",
       "         31847,  31906,  31920,  32353,  33404,  33545,  33553,  33667,  34315,\n",
       "         34925,  35973,  36110,  37365,  37648,  37705,  38025,  38504,  38889,\n",
       "         40362,  40528,  41106,  42292,  44218,  44314,  44490,  45228,  45236,\n",
       "         45611,  45785,  46063,  46984,  47021,  47023,  47809,  49900,  50368,\n",
       "         51020,  51313,  51727,  52033,  53476,  54295,  55556,  56137,  56364,\n",
       "         56600,  56673,  57090,  57185,  58700,  59078,  61397,  61855,  62616,\n",
       "         62626,  62712,  65233,  67016,  67453,  67827,  68073,  68568,  68775,\n",
       "         68879,  69886,  71969,  72457,  73360,  74287,  74290,  74510,  75999,\n",
       "         78130,  78631,  78717,  78785,  78959,  79069,  79718,  80220,  80256,\n",
       "         80955,  81991,  83959,  84176,  84715,  85832,  86849,  87538,  87861,\n",
       "         87885,  88769,  90467,  90719,  90946,  92190,  92199,  92654,  93133,\n",
       "         93437,  95261,  95590,  96230,  96801,  96836,  97332,  97346,  99004,\n",
       "         99254, 100975, 101607, 102314, 103839, 104026, 105074, 105179, 105423,\n",
       "        106047, 106102, 106254, 106603, 106833, 107410, 108338, 111865, 112500,\n",
       "        112675, 112964, 113339, 113978, 115061, 115144, 115360, 115480, 115668,\n",
       "        116776, 117032, 117312, 117625, 118081, 118922, 118977, 118979, 119274,\n",
       "        119281, 121055, 121149, 122056, 122066, 122441, 122667, 122983, 123363,\n",
       "        123841, 123878, 123990, 124027, 124389, 124541, 126045, 127271, 127584,\n",
       "        129096, 129790, 129811, 130332, 130703, 130806, 132454, 133867, 134603,\n",
       "        135591, 136023, 136352, 136726, 138260, 138430, 139575, 139648, 140181,\n",
       "        140445, 141099, 141233, 142181, 142415, 142770, 144580, 145006, 145019,\n",
       "        145105, 145108, 145485, 145523, 145919, 147300, 148277, 148375, 148746,\n",
       "        148760, 148901, 148928, 149155, 149210, 150858, 151236, 151632, 152578,\n",
       "        152673, 152833, 153119, 153148, 153290, 153759, 154515, 154940, 155140,\n",
       "        155826, 157151, 157332, 157575, 157953, 158199, 160961, 161522, 161734,\n",
       "        161748, 162518, 162933, 163113, 164763, 165000, 166156, 166352, 166381,\n",
       "        166837, 167397, 167798, 168228, 168917, 169525, 169558, 169734, 170082,\n",
       "        170290, 172694, 172773, 172972, 173024, 174608, 174640, 174862, 176776,\n",
       "        179539, 179893, 179941, 181693, 181954, 182038, 182211, 184051, 184886,\n",
       "        186339, 186488, 186606, 186794, 187234, 187281, 187469, 187775, 187799,\n",
       "        189783, 191190, 191668, 191699, 191833, 192192, 193019, 193741, 193823,\n",
       "        194100, 194536, 194648, 194660, 195541, 196275, 196563, 196819, 197781,\n",
       "        197889, 198133])"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (train_dataset.tensors[1] == 1).nonzero(as_tuple=True)[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "## Solution: use a different quantisation mechanism:"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Using fbgemm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 85,
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "import torch.optim as optim\n",
    "import torch.quantization\n",
    "\n",
    "class Net(nn.Module):\n",
    "    def __init__(self, num_features):\n",
    "        super(Net, self).__init__()\n",
    "        self.quant = torch.quantization.QuantStub()\n",
    "        self.fc1 = nn.Linear(num_features, 128)\n",
    "        self.fc2 = nn.Linear(128, 64)\n",
    "        self.fc3 = nn.Linear(64, 32)\n",
    "        self.fc4 = nn.Linear(32, 1)  # Output layer for binary classification\n",
    "        self.dequant = torch.quantization.DeQuantStub()\n",
    "\n",
    "    def forward(self, x):\n",
    "        x = self.quant(x)\n",
    "        x = torch.relu(self.fc1(x))\n",
    "        x = torch.relu(self.fc2(x))\n",
    "        x = torch.relu(self.fc3(x))\n",
    "        x = torch.sigmoid(self.fc4(x))  # Sigmoid activation for binary classification\n",
    "        x = self.dequant(x)\n",
    "        return x\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 86,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = Net(29)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "# Configure QAT\n",
    "model.qconfig = torch.quantization.get_default_qat_qconfig('fbgemm')\n",
    "model = torch.quantization.prepare_qat(model)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 88,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=29, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (fc3): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (fc4): Linear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([]), max_val=tensor([]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([1.]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=inf, max_val=-inf)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 88,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 89,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training\n",
    "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
    "criterion = nn.BCELoss()  # Binary Cross Entropy Loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 90,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch: 0\n",
      "========= Complete TrainLoss = 0.0841\n",
      "========= Complete TestLoss = 0.0867\n",
      "\n",
      "Epoch: 1\n",
      "========= Complete TrainLoss = 0.0408\n",
      "========= Complete TestLoss = 0.0423\n",
      "\n",
      "Epoch: 2\n",
      "========= Complete TrainLoss = 0.0223\n",
      "========= Complete TestLoss = 0.0239\n",
      "\n",
      "Epoch: 3\n",
      "========= Complete TrainLoss = 0.0102\n",
      "========= Complete TestLoss = 0.0111\n",
      "\n",
      "Epoch: 4\n",
      "========= Complete TrainLoss = 0.0085\n",
      "========= Complete TestLoss = 0.0089\n",
      "\n",
      "Epoch: 5\n",
      "========= Complete TrainLoss = 0.0085\n",
      "========= Complete TestLoss = 0.0091\n",
      "\n",
      "Epoch: 6\n",
      "========= Complete TrainLoss = 0.0077\n",
      "========= Complete TestLoss = 0.0083\n",
      "\n",
      "Epoch: 7\n",
      "========= Complete TrainLoss = 0.0079\n",
      "========= Complete TestLoss = 0.0084\n",
      "\n",
      "Epoch: 8\n",
      "========= Complete TrainLoss = 0.0079\n",
      "========= Complete TestLoss = 0.0087\n",
      "\n",
      "Epoch: 9\n",
      "========= Complete TrainLoss = 0.0074\n",
      "========= Complete TestLoss = 0.0081\n",
      "\n"
     ]
    }
   ],
   "source": [
    "best_loss = float('inf')\n",
    "loss_dict={\n",
    "        'train_loss':[],\n",
    "        'test_loss':[]\n",
    "}\n",
    "\n",
    "num_epochs=10\n",
    "for epoch in range(num_epochs):\n",
    "        model.train()\n",
    "        for i, (x_data, y) in enumerate(train_dataloader):\n",
    "\n",
    "                optimizer.zero_grad()\n",
    "                output = model(x_data)\n",
    "                loss = criterion(output, y)\n",
    "                loss.backward()\n",
    "                optimizer.step()\n",
    "        print(f\"Epoch: {epoch}\")\n",
    "        with torch.no_grad():\n",
    "                output = model(X_train_torch)\n",
    "                trainloss = criterion(output, y_train_torch)\n",
    "        print(f\"========= Complete TrainLoss = {trainloss.item():.4f}\")\n",
    "        \n",
    "\n",
    "        model.eval()\n",
    "        with torch.no_grad():\n",
    "                val_output = model(X_test_torch)\n",
    "                val_loss = criterion(val_output, y_test_torch)\n",
    "        print(f\"========= Complete TestLoss = {val_loss.item():.4f}\\n\")\n",
    "        loss_dict['test_loss'].append(val_loss.item())\n",
    "        \n",
    "        \n",
    "        # Early Stopping Code        \n",
    "        if val_loss < best_loss:\n",
    "                best_loss=val_loss\n",
    "                best_model_weights = copy.deepcopy(model.state_dict())\n",
    "                best_model = model\n",
    "                patience = 10 \n",
    "        else:\n",
    "                patience=-1\n",
    "                if patience==0:\n",
    "                        break;  \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 91,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert to int8\n",
    "model.eval()\n",
    "model.cpu()\n",
    "model_quant = torch.ao.quantization.convert(model)\n",
    "\n",
    "# Now you have a quantised binary classification model ready for inference\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([16.9117]), zero_point=tensor([1], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.89857292175293, max_val=2132.885009765625)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=29, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0035, 0.0037, 0.0040, 0.0054, 0.0026, 0.0038, 0.0042, 0.0011, 0.0035,\n",
       "              0.0039, 0.0043, 0.0063, 0.0017, 0.0056, 0.0036, 0.0025, 0.0015, 0.0025,\n",
       "              0.0040, 0.0051, 0.0022, 0.0030, 0.0035, 0.0034, 0.0049, 0.0042, 0.0020,\n",
       "              0.0049, 0.0025, 0.0049, 0.0060, 0.0045, 0.0039, 0.0027, 0.0063, 0.0043,\n",
       "              0.0025, 0.0035, 0.0033, 0.0020, 0.0043, 0.0034, 0.0033, 0.0041, 0.0033,\n",
       "              0.0057, 0.0046, 0.0025, 0.0044, 0.0043, 0.0040, 0.0049, 0.0015, 0.0033,\n",
       "              0.0043, 0.0021, 0.0032, 0.0040, 0.0016, 0.0015, 0.0059, 0.0034, 0.0017,\n",
       "              0.0041, 0.0020, 0.0064, 0.0028, 0.0029, 0.0047, 0.0038, 0.0024, 0.0024,\n",
       "              0.0043, 0.0035, 0.0037, 0.0052, 0.0016, 0.0027, 0.0050, 0.0032, 0.0037,\n",
       "              0.0048, 0.0044, 0.0021, 0.0022, 0.0022, 0.0052, 0.0020, 0.0021, 0.0014,\n",
       "              0.0061, 0.0029, 0.0040, 0.0050, 0.0037, 0.0018, 0.0042, 0.0017, 0.0022,\n",
       "              0.0040, 0.0042, 0.0046, 0.0041, 0.0016, 0.0043, 0.0058, 0.0055, 0.0028,\n",
       "              0.0029, 0.0037, 0.0025, 0.0030, 0.0045, 0.0040, 0.0053, 0.0064, 0.0023,\n",
       "              0.0066, 0.0023, 0.0034, 0.0037, 0.0048, 0.0019, 0.0054, 0.0029, 0.0040,\n",
       "              0.0027, 0.0053]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.3914, -0.3050, -0.4456, -0.6924, -0.3334, -0.3866, -0.5437, -0.1301,\n",
       "                -0.4530, -0.4950, -0.1531, -0.8021, -0.2128, -0.2461, -0.4151, -0.2265,\n",
       "                -0.1930, -0.3184, -0.5181, -0.6507, -0.2415, -0.3899, -0.1709, -0.3905,\n",
       "                -0.6285, -0.2625, -0.2235, -0.4750, -0.3147, -0.6326, -0.7624, -0.5724,\n",
       "                -0.4977, -0.1821, -0.8118, -0.5524, -0.2962, -0.4416, -0.4235, -0.2162,\n",
       "                -0.5558, -0.4319, -0.3003, -0.5304, -0.4073, -0.7276, -0.5455, -0.3229,\n",
       "                -0.5673, -0.5474, -0.5138, -0.6217, -0.1460, -0.4181, -0.2104, -0.1652,\n",
       "                -0.3303, -0.5122, -0.2028, -0.1883, -0.3300, -0.3766, -0.2226, -0.5254,\n",
       "                -0.2512, -0.8171, -0.1894, -0.2593, -0.6027, -0.3815, -0.1912, -0.3129,\n",
       "                -0.5530, -0.2782, -0.3503, -0.6654, -0.1562, -0.3479, -0.6428, -0.4056,\n",
       "                -0.1843, -0.2792, -0.2899, -0.1649, -0.1827, -0.1825, -0.6674, -0.2600,\n",
       "                -0.2673, -0.1856, -0.7759, -0.3657, -0.2420, -0.6371, -0.4754, -0.2351,\n",
       "                -0.2774, -0.1817, -0.2856, -0.5172, -0.5423, -0.4543, -0.5234, -0.1774,\n",
       "                -0.2359, -0.7448, -0.3608, -0.3520, -0.2641, -0.2668, -0.2333, -0.3396,\n",
       "                -0.2413, -0.4021, -0.6806, -0.8132, -0.3005, -0.8491, -0.2905, -0.4331,\n",
       "                -0.4726, -0.6158, -0.1929, -0.3917, -0.3715, -0.5115, -0.3492, -0.6839]), max_val=tensor([0.4503, 0.4655, 0.5105, 0.5425, 0.2341, 0.4881, 0.1860, 0.1381, 0.4328,\n",
       "                0.2320, 0.5415, 0.3485, 0.2010, 0.7083, 0.4519, 0.3231, 0.1543, 0.2328,\n",
       "                0.2421, 0.3889, 0.2770, 0.3287, 0.4448, 0.4330, 0.2238, 0.5303, 0.2540,\n",
       "                0.6180, 0.3019, 0.3833, 0.1491, 0.3701, 0.3848, 0.3445, 0.3210, 0.4911,\n",
       "                0.3139, 0.4415, 0.1191, 0.2520, 0.3651, 0.3284, 0.4167, 0.4163, 0.4234,\n",
       "                0.3728, 0.5847, 0.1661, 0.2987, 0.3569, 0.2003, 0.2700, 0.1943, 0.3539,\n",
       "                0.5522, 0.2648, 0.4074, 0.4119, 0.1140, 0.1532, 0.7433, 0.4376, 0.1708,\n",
       "                0.3543, 0.2379, 0.4132, 0.3508, 0.3624, 0.2409, 0.4870, 0.3022, 0.2421,\n",
       "                0.4064, 0.4462, 0.4684, 0.5180, 0.2032, 0.2745, 0.4352, 0.3595, 0.4728,\n",
       "                0.6119, 0.5560, 0.2614, 0.2774, 0.2810, 0.2164, 0.2516, 0.1701, 0.1365,\n",
       "                0.2640, 0.2057, 0.5135, 0.2747, 0.4686, 0.2310, 0.5327, 0.2135, 0.2690,\n",
       "                0.4048, 0.4435, 0.5792, 0.1400, 0.2034, 0.5462, 0.2565, 0.6945, 0.3415,\n",
       "                0.3683, 0.4695, 0.3237, 0.3867, 0.5671, 0.5037, 0.2969, 0.2589, 0.1942,\n",
       "                0.3477, 0.2885, 0.2662, 0.2930, 0.5957, 0.2457, 0.6820, 0.2976, 0.2441,\n",
       "                0.3382, 0.1367])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.2088]), zero_point=tensor([88], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-547.3888549804688, max_val=241.1300048828125)\n",
       "    )\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0018, 0.0052, 0.0035, 0.0052, 0.0041, 0.0026, 0.0064, 0.0007,\n",
       "              0.0022, 0.0046, 0.0008, 0.0029, 0.0038, 0.0010, 0.0009, 0.0058, 0.0009,\n",
       "              0.0076, 0.0011, 0.0021, 0.0007, 0.0007, 0.0046, 0.0042, 0.0032, 0.0013,\n",
       "              0.0009, 0.0029, 0.0061, 0.0043, 0.0007, 0.0036, 0.0060, 0.0035, 0.0043,\n",
       "              0.0008, 0.0059, 0.0012, 0.0007, 0.0014, 0.0013, 0.0049, 0.0044, 0.0008,\n",
       "              0.0015, 0.0051, 0.0038, 0.0016, 0.0024, 0.0055, 0.0041, 0.0037, 0.0033,\n",
       "              0.0008, 0.0033, 0.0007, 0.0009, 0.0023, 0.0009, 0.0019, 0.0065, 0.0027,\n",
       "              0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.1172, -0.2351, -0.2832, -0.2310, -0.2411, -0.2027, -0.1589, -0.2923,\n",
       "                -0.0905, -0.1120, -0.0952, -0.1071, -0.2951, -0.2941, -0.1247, -0.1207,\n",
       "                -0.2441, -0.1112, -0.2968, -0.1462, -0.2637, -0.0915, -0.0867, -0.2502,\n",
       "                -0.3617, -0.2496, -0.1727, -0.1094, -0.2486, -0.2235, -0.3732, -0.0927,\n",
       "                -0.2492, -0.2810, -0.4469, -0.2835, -0.0988, -0.2177, -0.1543, -0.0891,\n",
       "                -0.1796, -0.1710, -0.6226, -0.1523, -0.0996, -0.1898, -0.2719, -0.2339,\n",
       "                -0.1703, -0.1967, -0.1559, -0.5248, -0.3737, -0.4021, -0.0937, -0.2357,\n",
       "                -0.0870, -0.1134, -0.1618, -0.1142, -0.1415, -0.4792, -0.1605, -0.1468]), max_val=tensor([0.1049, 0.2119, 0.6667, 0.4496, 0.6596, 0.5230, 0.3299, 0.8067, 0.0949,\n",
       "                0.2852, 0.5799, 0.0791, 0.3663, 0.4784, 0.1229, 0.0945, 0.7352, 0.0858,\n",
       "                0.9700, 0.0973, 0.2719, 0.0893, 0.0824, 0.5828, 0.5276, 0.4041, 0.1044,\n",
       "                0.0957, 0.3635, 0.7740, 0.5477, 0.0859, 0.4577, 0.7670, 0.3154, 0.5413,\n",
       "                0.0848, 0.7484, 0.1325, 0.0863, 0.1297, 0.1389, 0.5582, 0.5637, 0.0813,\n",
       "                0.1216, 0.6420, 0.4803, 0.2017, 0.3025, 0.6998, 0.4521, 0.4749, 0.4196,\n",
       "                0.0958, 0.4130, 0.0880, 0.0803, 0.2941, 0.0925, 0.2439, 0.8205, 0.3424,\n",
       "                0.1661])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.8616]), zero_point=tensor([22], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-108.83106994628906, max_val=508.5890808105469)\n",
       "    )\n",
       "  )\n",
       "  (fc3): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0027, 0.0049, 0.0053, 0.0031, 0.0017, 0.0024, 0.0046, 0.0029, 0.0022,\n",
       "              0.0030, 0.0010, 0.0027, 0.0049, 0.0019, 0.0043, 0.0013, 0.0029, 0.0022,\n",
       "              0.0030, 0.0025, 0.0015, 0.0040, 0.0025, 0.0014, 0.0010, 0.0026, 0.0019,\n",
       "              0.0015, 0.0032, 0.0013, 0.0023, 0.0058]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.3482, -0.2915, -0.3734, -0.2699, -0.2072, -0.2014, -0.2590, -0.1992,\n",
       "                -0.2833, -0.3854, -0.1264, -0.2363, -0.3062, -0.2389, -0.5544, -0.1691,\n",
       "                -0.2405, -0.2845, -0.2308, -0.3152, -0.1938, -0.5091, -0.3192, -0.1821,\n",
       "                -0.1241, -0.3358, -0.2378, -0.1928, -0.4059, -0.1605, -0.2899, -0.4584]), max_val=tensor([0.2439, 0.6161, 0.6730, 0.3877, 0.2205, 0.3043, 0.5880, 0.3674, 0.2082,\n",
       "                0.3322, 0.1149, 0.3459, 0.6198, 0.1540, 0.1899, 0.1055, 0.3695, 0.1948,\n",
       "                0.3835, 0.1402, 0.1167, 0.4308, 0.1830, 0.1239, 0.1243, 0.1543, 0.1681,\n",
       "                0.1237, 0.1599, 0.1622, 0.1755, 0.7328])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.9591]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-348.84942626953125, max_val=280.96063232421875)\n",
       "    )\n",
       "  )\n",
       "  (fc4): Linear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2674]), max_val=tensor([0.1677]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2398]), zero_point=tensor([127], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-30.451454162597656, max_val=-0.9081704616546631)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 92,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): Quantize(scale=tensor([16.9117]), zero_point=tensor([1]), dtype=torch.quint8)\n",
       "  (fc1): QuantizedLinear(in_features=29, out_features=128, scale=6.208809852600098, zero_point=88, qscheme=torch.per_channel_affine)\n",
       "  (fc2): QuantizedLinear(in_features=128, out_features=64, scale=4.861576080322266, zero_point=22, qscheme=torch.per_channel_affine)\n",
       "  (fc3): QuantizedLinear(in_features=64, out_features=32, scale=4.959134101867676, zero_point=70, qscheme=torch.per_channel_affine)\n",
       "  (fc4): QuantizedLinear(in_features=32, out_features=1, scale=0.23977522552013397, zero_point=127, qscheme=torch.per_channel_affine)\n",
       "  (dequant): DeQuantize()\n",
       ")"
      ]
     },
     "execution_count": 93,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 94,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 1.9738, -0.2370, -0.2398,  0.4631, -0.6509, -0.7381, -0.3787, -0.1465,\n",
       "         1.1360, -0.1925, -0.6413,  1.0334,  1.0345, -0.2216,  0.5477,  0.1455,\n",
       "        -0.5923, -0.3133, -0.0873, -0.1571, -0.1655, -0.2883,  0.3579,  0.0276,\n",
       "        -0.3998, -0.6285,  0.0404, -0.0276, 12.4900])"
      ]
     },
     "execution_count": 94,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "train_dataloader.dataset[120][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 95,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, 16.7436])"
      ]
     },
     "execution_count": 95,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.quant\n",
    "model.quant(train_dataloader.dataset[120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 96,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([ 0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,  0.0000,\n",
       "         0.0000,  0.0000,  0.0000,  0.0000, 16.9117], size=(29,),\n",
       "       dtype=torch.quint8, quantization_scheme=torch.per_tensor_affine,\n",
       "       scale=16.91168212890625, zero_point=1)"
      ]
     },
     "execution_count": 96,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# model.quant\n",
    "model_quant.quant(train_dataloader.dataset[120][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 97,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.5808,   0.0000, -16.5808,  16.5808, -16.5808,   0.0000, -16.5808,\n",
       "          0.0000,   0.0000, -16.5808,   0.0000, -16.5808,   0.0000, -16.5808,\n",
       "          0.0000, -16.5808, -16.5808,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         33.1615])"
      ]
     },
     "execution_count": 97,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.quant(train_dataloader.dataset[198133][0])\n",
    "# When I run this, the values will be slightly different because of MovingAverage concept in activation_post_process as per fbgemm algorithm\n",
    "# This runs the quantisation and dequantisation immediately which is the typical process for each input in QAT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 98,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.4196,   0.0000, -16.4196,  16.4196, -16.4196,   0.0000, -16.4196,\n",
       "          0.0000,   0.0000, -16.4196,   0.0000, -16.4196,   0.0000, -16.4196,\n",
       "          0.0000, -16.4196, -16.4196,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         32.8391])"
      ]
     },
     "execution_count": 98,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.quant(train_dataloader.dataset[198133][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 99,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-16.2600,   0.0000, -16.2600,  16.2600, -16.2600,   0.0000, -16.2600,\n",
       "          0.0000,   0.0000, -16.2600,   0.0000, -16.2600,   0.0000, -16.2600,\n",
       "          0.0000, -16.2600, -16.2600,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "         32.5199])"
      ]
     },
     "execution_count": 99,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.quant(train_dataloader.dataset[198133][0])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 100,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-16.9117,   0.0000, -16.9117,  16.9117, -16.9117,   0.0000, -16.9117,\n",
       "           0.0000,   0.0000, -16.9117,   0.0000, -16.9117,   0.0000, -16.9117,\n",
       "           0.0000, -16.9117, -16.9117,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "           0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,   0.0000,\n",
       "          33.8234]], size=(1, 29), dtype=torch.quint8,\n",
       "       quantization_scheme=torch.per_tensor_affine, scale=16.91168212890625,\n",
       "       zero_point=1)"
      ]
     },
     "execution_count": 100,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.quant(train_dataset.tensors[0:1][0][198133:198133+1])\n",
    "# These values are constant as we have already created this qwuantised model using \"model\" and this has fixed the values of s and z for activation each layer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 101,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Net(\n",
       "  (quant): QuantStub(\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([16.2600]), zero_point=tensor([1], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-14.943807601928711, max_val=2050.070556640625)\n",
       "    )\n",
       "  )\n",
       "  (fc1): Linear(\n",
       "    in_features=29, out_features=128, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0035, 0.0037, 0.0040, 0.0054, 0.0026, 0.0038, 0.0042, 0.0011, 0.0035,\n",
       "              0.0039, 0.0043, 0.0063, 0.0017, 0.0056, 0.0036, 0.0025, 0.0015, 0.0025,\n",
       "              0.0040, 0.0051, 0.0022, 0.0030, 0.0035, 0.0034, 0.0049, 0.0042, 0.0020,\n",
       "              0.0049, 0.0025, 0.0049, 0.0060, 0.0045, 0.0039, 0.0027, 0.0063, 0.0043,\n",
       "              0.0025, 0.0035, 0.0033, 0.0020, 0.0043, 0.0034, 0.0033, 0.0041, 0.0033,\n",
       "              0.0057, 0.0046, 0.0025, 0.0044, 0.0043, 0.0040, 0.0049, 0.0015, 0.0033,\n",
       "              0.0043, 0.0021, 0.0032, 0.0040, 0.0016, 0.0015, 0.0059, 0.0034, 0.0017,\n",
       "              0.0041, 0.0020, 0.0064, 0.0028, 0.0029, 0.0047, 0.0038, 0.0024, 0.0024,\n",
       "              0.0043, 0.0035, 0.0037, 0.0052, 0.0016, 0.0027, 0.0050, 0.0032, 0.0037,\n",
       "              0.0048, 0.0044, 0.0021, 0.0022, 0.0022, 0.0052, 0.0020, 0.0021, 0.0014,\n",
       "              0.0061, 0.0029, 0.0040, 0.0050, 0.0037, 0.0018, 0.0042, 0.0017, 0.0022,\n",
       "              0.0040, 0.0042, 0.0046, 0.0041, 0.0016, 0.0043, 0.0058, 0.0055, 0.0028,\n",
       "              0.0029, 0.0037, 0.0025, 0.0030, 0.0045, 0.0040, 0.0053, 0.0064, 0.0023,\n",
       "              0.0066, 0.0023, 0.0034, 0.0037, 0.0048, 0.0019, 0.0054, 0.0029, 0.0040,\n",
       "              0.0027, 0.0053]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.3914, -0.3050, -0.4456, -0.6924, -0.3334, -0.3866, -0.5437, -0.1301,\n",
       "                -0.4530, -0.4950, -0.1531, -0.8021, -0.2128, -0.2461, -0.4151, -0.2265,\n",
       "                -0.1930, -0.3184, -0.5181, -0.6507, -0.2415, -0.3899, -0.1709, -0.3905,\n",
       "                -0.6285, -0.2625, -0.2235, -0.4750, -0.3147, -0.6326, -0.7624, -0.5724,\n",
       "                -0.4977, -0.1821, -0.8118, -0.5524, -0.2962, -0.4416, -0.4235, -0.2162,\n",
       "                -0.5558, -0.4319, -0.3003, -0.5304, -0.4073, -0.7276, -0.5455, -0.3229,\n",
       "                -0.5673, -0.5474, -0.5138, -0.6217, -0.1460, -0.4181, -0.2104, -0.1652,\n",
       "                -0.3303, -0.5122, -0.2028, -0.1883, -0.3300, -0.3766, -0.2226, -0.5254,\n",
       "                -0.2512, -0.8171, -0.1894, -0.2593, -0.6027, -0.3815, -0.1912, -0.3129,\n",
       "                -0.5530, -0.2782, -0.3503, -0.6654, -0.1562, -0.3479, -0.6428, -0.4056,\n",
       "                -0.1843, -0.2792, -0.2899, -0.1649, -0.1827, -0.1825, -0.6674, -0.2600,\n",
       "                -0.2673, -0.1856, -0.7759, -0.3657, -0.2420, -0.6371, -0.4754, -0.2351,\n",
       "                -0.2774, -0.1817, -0.2856, -0.5172, -0.5423, -0.4543, -0.5234, -0.1774,\n",
       "                -0.2359, -0.7448, -0.3608, -0.3520, -0.2641, -0.2668, -0.2333, -0.3396,\n",
       "                -0.2413, -0.4021, -0.6806, -0.8132, -0.3005, -0.8491, -0.2905, -0.4331,\n",
       "                -0.4726, -0.6158, -0.1929, -0.3917, -0.3715, -0.5115, -0.3492, -0.6839]), max_val=tensor([0.4503, 0.4655, 0.5105, 0.5425, 0.2341, 0.4881, 0.1860, 0.1381, 0.4328,\n",
       "                0.2320, 0.5415, 0.3485, 0.2010, 0.7083, 0.4519, 0.3231, 0.1543, 0.2328,\n",
       "                0.2421, 0.3889, 0.2770, 0.3287, 0.4448, 0.4330, 0.2238, 0.5303, 0.2540,\n",
       "                0.6180, 0.3019, 0.3833, 0.1491, 0.3701, 0.3848, 0.3445, 0.3210, 0.4911,\n",
       "                0.3139, 0.4415, 0.1191, 0.2520, 0.3651, 0.3284, 0.4167, 0.4163, 0.4234,\n",
       "                0.3728, 0.5847, 0.1661, 0.2987, 0.3569, 0.2003, 0.2700, 0.1943, 0.3539,\n",
       "                0.5522, 0.2648, 0.4074, 0.4119, 0.1140, 0.1532, 0.7433, 0.4376, 0.1708,\n",
       "                0.3543, 0.2379, 0.4132, 0.3508, 0.3624, 0.2409, 0.4870, 0.3022, 0.2421,\n",
       "                0.4064, 0.4462, 0.4684, 0.5180, 0.2032, 0.2745, 0.4352, 0.3595, 0.4728,\n",
       "                0.6119, 0.5560, 0.2614, 0.2774, 0.2810, 0.2164, 0.2516, 0.1701, 0.1365,\n",
       "                0.2640, 0.2057, 0.5135, 0.2747, 0.4686, 0.2310, 0.5327, 0.2135, 0.2690,\n",
       "                0.4048, 0.4435, 0.5792, 0.1400, 0.2034, 0.5462, 0.2565, 0.6945, 0.3415,\n",
       "                0.3683, 0.4695, 0.3237, 0.3867, 0.5671, 0.5037, 0.2969, 0.2589, 0.1942,\n",
       "                0.3477, 0.2885, 0.2662, 0.2930, 0.5957, 0.2457, 0.6820, 0.2976, 0.2441,\n",
       "                0.3382, 0.1367])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([6.2088]), zero_point=tensor([88], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-547.3888549804688, max_val=241.1300048828125)\n",
       "    )\n",
       "  )\n",
       "  (fc2): Linear(\n",
       "    in_features=128, out_features=64, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0009, 0.0018, 0.0052, 0.0035, 0.0052, 0.0041, 0.0026, 0.0064, 0.0007,\n",
       "              0.0022, 0.0046, 0.0008, 0.0029, 0.0038, 0.0010, 0.0009, 0.0058, 0.0009,\n",
       "              0.0076, 0.0011, 0.0021, 0.0007, 0.0007, 0.0046, 0.0042, 0.0032, 0.0013,\n",
       "              0.0009, 0.0029, 0.0061, 0.0043, 0.0007, 0.0036, 0.0060, 0.0035, 0.0043,\n",
       "              0.0008, 0.0059, 0.0012, 0.0007, 0.0014, 0.0013, 0.0049, 0.0044, 0.0008,\n",
       "              0.0015, 0.0051, 0.0038, 0.0016, 0.0024, 0.0055, 0.0041, 0.0037, 0.0033,\n",
       "              0.0008, 0.0033, 0.0007, 0.0009, 0.0023, 0.0009, 0.0019, 0.0065, 0.0027,\n",
       "              0.0013]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.1172, -0.2351, -0.2832, -0.2310, -0.2411, -0.2027, -0.1589, -0.2923,\n",
       "                -0.0905, -0.1120, -0.0952, -0.1071, -0.2951, -0.2941, -0.1247, -0.1207,\n",
       "                -0.2441, -0.1112, -0.2968, -0.1462, -0.2637, -0.0915, -0.0867, -0.2502,\n",
       "                -0.3617, -0.2496, -0.1727, -0.1094, -0.2486, -0.2235, -0.3732, -0.0927,\n",
       "                -0.2492, -0.2810, -0.4469, -0.2835, -0.0988, -0.2177, -0.1543, -0.0891,\n",
       "                -0.1796, -0.1710, -0.6226, -0.1523, -0.0996, -0.1898, -0.2719, -0.2339,\n",
       "                -0.1703, -0.1967, -0.1559, -0.5248, -0.3737, -0.4021, -0.0937, -0.2357,\n",
       "                -0.0870, -0.1134, -0.1618, -0.1142, -0.1415, -0.4792, -0.1605, -0.1468]), max_val=tensor([0.1049, 0.2119, 0.6667, 0.4496, 0.6596, 0.5230, 0.3299, 0.8067, 0.0949,\n",
       "                0.2852, 0.5799, 0.0791, 0.3663, 0.4784, 0.1229, 0.0945, 0.7352, 0.0858,\n",
       "                0.9700, 0.0973, 0.2719, 0.0893, 0.0824, 0.5828, 0.5276, 0.4041, 0.1044,\n",
       "                0.0957, 0.3635, 0.7740, 0.5477, 0.0859, 0.4577, 0.7670, 0.3154, 0.5413,\n",
       "                0.0848, 0.7484, 0.1325, 0.0863, 0.1297, 0.1389, 0.5582, 0.5637, 0.0813,\n",
       "                0.1216, 0.6420, 0.4803, 0.2017, 0.3025, 0.6998, 0.4521, 0.4749, 0.4196,\n",
       "                0.0958, 0.4130, 0.0880, 0.0803, 0.2941, 0.0925, 0.2439, 0.8205, 0.3424,\n",
       "                0.1661])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.8616]), zero_point=tensor([22], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-108.83106994628906, max_val=508.5890808105469)\n",
       "    )\n",
       "  )\n",
       "  (fc3): Linear(\n",
       "    in_features=64, out_features=32, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0027, 0.0049, 0.0053, 0.0031, 0.0017, 0.0024, 0.0046, 0.0029, 0.0022,\n",
       "              0.0030, 0.0010, 0.0027, 0.0049, 0.0019, 0.0043, 0.0013, 0.0029, 0.0022,\n",
       "              0.0030, 0.0025, 0.0015, 0.0040, 0.0025, 0.0014, 0.0010, 0.0026, 0.0019,\n",
       "              0.0015, 0.0032, 0.0013, 0.0023, 0.0058]), zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "              0, 0, 0, 0, 0, 0, 0, 0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(\n",
       "        min_val=tensor([-0.3482, -0.2915, -0.3734, -0.2699, -0.2072, -0.2014, -0.2590, -0.1992,\n",
       "                -0.2833, -0.3854, -0.1264, -0.2363, -0.3062, -0.2389, -0.5544, -0.1691,\n",
       "                -0.2405, -0.2845, -0.2308, -0.3152, -0.1938, -0.5091, -0.3192, -0.1821,\n",
       "                -0.1241, -0.3358, -0.2378, -0.1928, -0.4059, -0.1605, -0.2899, -0.4584]), max_val=tensor([0.2439, 0.6161, 0.6730, 0.3877, 0.2205, 0.3043, 0.5880, 0.3674, 0.2082,\n",
       "                0.3322, 0.1149, 0.3459, 0.6198, 0.1540, 0.1899, 0.1055, 0.3695, 0.1948,\n",
       "                0.3835, 0.1402, 0.1167, 0.4308, 0.1830, 0.1239, 0.1243, 0.1543, 0.1681,\n",
       "                0.1237, 0.1599, 0.1622, 0.1755, 0.7328])\n",
       "      )\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([4.9591]), zero_point=tensor([70], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-348.84942626953125, max_val=280.96063232421875)\n",
       "    )\n",
       "  )\n",
       "  (fc4): Linear(\n",
       "    in_features=32, out_features=1, bias=True\n",
       "    (weight_fake_quant): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.0021]), zero_point=tensor([0], dtype=torch.int32), dtype=torch.qint8, quant_min=-128, quant_max=127, qscheme=torch.per_channel_symmetric, reduce_range=False\n",
       "      (activation_post_process): MovingAveragePerChannelMinMaxObserver(min_val=tensor([-0.2674]), max_val=tensor([0.1677]))\n",
       "    )\n",
       "    (activation_post_process): FusedMovingAvgObsFakeQuantize(\n",
       "      fake_quant_enabled=tensor([1]), observer_enabled=tensor([1]), scale=tensor([0.2398]), zero_point=tensor([127], dtype=torch.int32), dtype=torch.quint8, quant_min=0, quant_max=127, qscheme=torch.per_tensor_affine, reduce_range=True\n",
       "      (activation_post_process): MovingAverageMinMaxObserver(min_val=-30.451454162597656, max_val=-0.9081704616546631)\n",
       "    )\n",
       "  )\n",
       "  (dequant): DeQuantStub()\n",
       ")"
      ]
     },
     "execution_count": 101,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.5000], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 102,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][198133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.6121], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 104,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][198133])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 105,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.7115], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 105,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][198133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 106,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.8270], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 106,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][198133])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#question: How is model getting better here with every iteration ? -- Maybe due to moving average getting optimised for this input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.9961]])"
      ]
     },
     "execution_count": 103,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.forward(train_dataset.tensors[0:1][0][198133:198133+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 111,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0004], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 111,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][3802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 112,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0005], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 112,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][3802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 113,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0005], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 113,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][3802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 114,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([0.0004], grad_fn=<SigmoidBackward0>)"
      ]
     },
     "execution_count": 114,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.forward(train_dataset.tensors[0:1][0][3802])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 115,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[0.]])"
      ]
     },
     "execution_count": 115,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.forward(train_dataset.tensors[0:1][0][3802:3802+1])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 107,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([  1645,   2850,   3260,   3714,   3802,   3854,   3891,   4032,   4445,\n",
       "          4557,   5607,   6681,   7452,   7748,   8020,  10130,  10207,  10752,\n",
       "         11702,  11891,  13150,  13462,  14836,  15513,  16184,  17138,  18179,\n",
       "         19013,  19020,  20284,  20618,  20794,  21526,  21757,  21797,  21833,\n",
       "         22028,  22979,  23133,  23134,  23504,  24057,  24266,  25626,  25792,\n",
       "         26150,  27258,  27507,  27661,  28342,  28760,  29306,  30713,  31760,\n",
       "         31847,  31906,  31920,  32353,  33404,  33545,  33553,  33667,  34315,\n",
       "         34925,  35973,  36110,  37365,  37648,  37705,  38025,  38504,  38889,\n",
       "         40362,  40528,  41106,  42292,  44218,  44314,  44490,  45228,  45236,\n",
       "         45611,  45785,  46063,  46984,  47021,  47023,  47809,  49900,  50368,\n",
       "         51020,  51313,  51727,  52033,  53476,  54295,  55556,  56137,  56364,\n",
       "         56600,  56673,  57090,  57185,  58700,  59078,  61397,  61855,  62616,\n",
       "         62626,  62712,  65233,  67016,  67453,  67827,  68073,  68568,  68775,\n",
       "         68879,  69886,  71969,  72457,  73360,  74287,  74290,  74510,  75999,\n",
       "         78130,  78631,  78717,  78785,  78959,  79069,  79718,  80220,  80256,\n",
       "         80955,  81991,  83959,  84176,  84715,  85832,  86849,  87538,  87861,\n",
       "         87885,  88769,  90467,  90719,  90946,  92190,  92199,  92654,  93133,\n",
       "         93437,  95261,  95590,  96230,  96801,  96836,  97332,  97346,  99004,\n",
       "         99254, 100975, 101607, 102314, 103839, 104026, 105074, 105179, 105423,\n",
       "        106047, 106102, 106254, 106603, 106833, 107410, 108338, 111865, 112500,\n",
       "        112675, 112964, 113339, 113978, 115061, 115144, 115360, 115480, 115668,\n",
       "        116776, 117032, 117312, 117625, 118081, 118922, 118977, 118979, 119274,\n",
       "        119281, 121055, 121149, 122056, 122066, 122441, 122667, 122983, 123363,\n",
       "        123841, 123878, 123990, 124027, 124389, 124541, 126045, 127271, 127584,\n",
       "        129096, 129790, 129811, 130332, 130703, 130806, 132454, 133867, 134603,\n",
       "        135591, 136023, 136352, 136726, 138260, 138430, 139575, 139648, 140181,\n",
       "        140445, 141099, 141233, 142181, 142415, 142770, 144580, 145006, 145019,\n",
       "        145105, 145108, 145485, 145523, 145919, 147300, 148277, 148375, 148746,\n",
       "        148760, 148901, 148928, 149155, 149210, 150858, 151236, 151632, 152578,\n",
       "        152673, 152833, 153119, 153148, 153290, 153759, 154515, 154940, 155140,\n",
       "        155826, 157151, 157332, 157575, 157953, 158199, 160961, 161522, 161734,\n",
       "        161748, 162518, 162933, 163113, 164763, 165000, 166156, 166352, 166381,\n",
       "        166837, 167397, 167798, 168228, 168917, 169525, 169558, 169734, 170082,\n",
       "        170290, 172694, 172773, 172972, 173024, 174608, 174640, 174862, 176776,\n",
       "        179539, 179893, 179941, 181693, 181954, 182038, 182211, 184051, 184886,\n",
       "        186339, 186488, 186606, 186794, 187234, 187281, 187469, 187775, 187799,\n",
       "        189783, 191190, 191668, 191699, 191833, 192192, 193019, 193741, 193823,\n",
       "        194100, 194536, 194648, 194660, 195541, 196275, 196563, 196819, 197781,\n",
       "        197889, 198133])"
      ]
     },
     "execution_count": 107,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "indices = (train_dataset.tensors[1] == 1).nonzero(as_tuple=True)[0]\n",
    "indices"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 110,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Parameter containing:\n",
       "tensor([[-0.1574, -0.0457,  0.1720,  ..., -0.1600,  0.0196, -0.1058],\n",
       "        [ 0.2330, -0.1326,  0.2652,  ..., -0.2555,  0.2322, -0.0050],\n",
       "        [-0.3567, -0.0350, -0.2258,  ...,  0.0405, -0.4464, -0.0431],\n",
       "        ...,\n",
       "        [-0.3744, -0.0697, -0.2622,  ...,  0.0819, -0.1119,  0.0328],\n",
       "        [ 0.3383, -0.2358, -0.0510,  ...,  0.1171,  0.0038, -0.3143],\n",
       "        [ 0.0226,  0.0032,  0.0469,  ...,  0.1110, -0.0615,  0.0186]],\n",
       "       requires_grad=True)"
      ]
     },
     "execution_count": 110,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 118,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[-0.1589, -0.0459,  0.1731,  ..., -0.1589,  0.0212, -0.1060],\n",
       "        [ 0.2337, -0.1314,  0.2665,  ..., -0.2556,  0.2337, -0.0037],\n",
       "        [-0.3563, -0.0360, -0.2242,  ...,  0.0400, -0.4444, -0.0440],\n",
       "        ...,\n",
       "        [-0.3732, -0.0682, -0.2608,  ...,  0.0803, -0.1124,  0.0321],\n",
       "        [ 0.3396, -0.2355, -0.0520,  ...,  0.1178,  0.0027, -0.3150],\n",
       "        [ 0.0215,  0.0054,  0.0483,  ...,  0.1126, -0.0590,  0.0161]],\n",
       "       size=(128, 29), dtype=torch.qint8,\n",
       "       quantization_scheme=torch.per_channel_affine,\n",
       "       scale=tensor([0.0035, 0.0037, 0.0040, 0.0054, 0.0026, 0.0038, 0.0043, 0.0011, 0.0036,\n",
       "        0.0039, 0.0042, 0.0063, 0.0017, 0.0056, 0.0035, 0.0025, 0.0015, 0.0025,\n",
       "        0.0041, 0.0051, 0.0022, 0.0031, 0.0035, 0.0034, 0.0049, 0.0042, 0.0020,\n",
       "        0.0048, 0.0025, 0.0050, 0.0060, 0.0045, 0.0039, 0.0027, 0.0064, 0.0043,\n",
       "        0.0025, 0.0035, 0.0033, 0.0020, 0.0044, 0.0034, 0.0033, 0.0042, 0.0033,\n",
       "        0.0057, 0.0046, 0.0025, 0.0045, 0.0043, 0.0040, 0.0049, 0.0015, 0.0033,\n",
       "        0.0043, 0.0021, 0.0032, 0.0040, 0.0016, 0.0015, 0.0058, 0.0034, 0.0017,\n",
       "        0.0041, 0.0020, 0.0064, 0.0028, 0.0028, 0.0047, 0.0038, 0.0024, 0.0025,\n",
       "        0.0043, 0.0035, 0.0037, 0.0052, 0.0016, 0.0027, 0.0050, 0.0032, 0.0037,\n",
       "        0.0048, 0.0044, 0.0021, 0.0022, 0.0022, 0.0052, 0.0020, 0.0021, 0.0015,\n",
       "        0.0061, 0.0029, 0.0040, 0.0050, 0.0037, 0.0018, 0.0042, 0.0017, 0.0022,\n",
       "        0.0041, 0.0043, 0.0045, 0.0041, 0.0016, 0.0043, 0.0058, 0.0054, 0.0028,\n",
       "        0.0029, 0.0037, 0.0025, 0.0030, 0.0044, 0.0040, 0.0053, 0.0064, 0.0024,\n",
       "        0.0067, 0.0023, 0.0034, 0.0037, 0.0048, 0.0019, 0.0053, 0.0029, 0.0040,\n",
       "        0.0027, 0.0054], dtype=torch.float64),\n",
       "       zero_point=tensor([0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0, 0,\n",
       "        0, 0, 0, 0, 0, 0, 0, 0]),\n",
       "       axis=0)"
      ]
     },
     "execution_count": 118,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.fc1.weight()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([-0.1589, -0.0459,  0.1731,  0.2049, -0.1837, -0.3921, -0.0989, -0.2084,\n",
       "         0.1695, -0.1095,  0.0459, -0.1166, -0.1307,  0.2225, -0.1060,  0.0106,\n",
       "         0.0918,  0.1731, -0.1130, -0.2119,  0.4486, -0.2437, -0.1766,  0.1625,\n",
       "         0.0989, -0.0459, -0.1589,  0.0212, -0.1060], size=(29,),\n",
       "       dtype=torch.qint8, quantization_scheme=torch.per_tensor_affine,\n",
       "       scale=0.0035321766044944525, zero_point=0)"
      ]
     },
     "execution_count": 140,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model_quant.fc1.weight()[0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "## how did we get this scale value of scale=0.0035321766044944525 ?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(tensor(0.4512, grad_fn=<MaxBackward1>),\n",
       " tensor(-0.3913, grad_fn=<MinBackward1>))"
      ]
     },
     "execution_count": 142,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "model.fc1.weight[0].max(), model.fc1.weight[0].min()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.003552755905511811"
      ]
     },
     "execution_count": 144,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "s = (.4512)/(2**7-1)\n",
    "s\n",
    "\n",
    "# It is using symmetric quantisation, so z is 0\n",
    "# # Every neuron of each layer row has its own s and z value. Here we have calculated for fc1 weight()[0] i.e 1st neuron"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "-45.400000000000006"
      ]
     },
     "execution_count": 145,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Now, we use this s and actual weight value to find the quantised weight value:\n",
    "    \n",
    "-0.1589/.0035"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 109,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "tensor([[ -45,  -13,   49,  ...,  -45,    6,  -30],\n",
       "        [  64,  -36,   73,  ...,  -70,   64,   -1],\n",
       "        [ -89,   -9,  -56,  ...,   10, -111,  -11],\n",
       "        ...,\n",
       "        [ -93,  -17,  -65,  ...,   20,  -28,    8],\n",
       "        [ 124,  -86,  -19,  ...,   43,    1, -115],\n",
       "        [   4,    1,    9,  ...,   21,  -11,    3]], dtype=torch.int8)"
      ]
     },
     "execution_count": 109,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "torch.int_repr(model_quant.fc1.weight())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "python38",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.18"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
