{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "provenance": [],
      "collapsed_sections": [
        "6gm4jsGgFs8M",
        "YJjK95cLF5q8",
        "VoSpnr14FuFV"
      ]
    },
    "kernelspec": {
      "name": "python3",
      "display_name": "Python 3"
    },
    "language_info": {
      "name": "python"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "source": [
        "## Basic of Langchain and its components:\n",
        "* ChatModel\n",
        "* Messages\n",
        "* ChatPromptTemplate\n",
        "* Output Parser\n",
        "* Chain"
      ],
      "metadata": {
        "id": "6gm4jsGgFs8M"
      }
    },
    {
      "cell_type": "code",
      "execution_count": 1,
      "metadata": {
        "id": "znkBQG8CuuYj",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "outputId": "9856cb2c-06c2-4a08-9fb0-5905886c2d75"
      },
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Collecting langchain\n",
            "  Downloading langchain-0.3.3-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting langchain-openai\n",
            "  Downloading langchain_openai-0.2.2-py3-none-any.whl.metadata (2.6 kB)\n",
            "Requirement already satisfied: PyYAML>=5.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (6.0.2)\n",
            "Requirement already satisfied: SQLAlchemy<3,>=1.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.0.35)\n",
            "Requirement already satisfied: aiohttp<4.0.0,>=3.8.3 in /usr/local/lib/python3.10/dist-packages (from langchain) (3.10.8)\n",
            "Requirement already satisfied: async-timeout<5.0.0,>=4.0.0 in /usr/local/lib/python3.10/dist-packages (from langchain) (4.0.3)\n",
            "Collecting langchain-core<0.4.0,>=0.3.10 (from langchain)\n",
            "  Downloading langchain_core-0.3.10-py3-none-any.whl.metadata (6.3 kB)\n",
            "Collecting langchain-text-splitters<0.4.0,>=0.3.0 (from langchain)\n",
            "  Downloading langchain_text_splitters-0.3.0-py3-none-any.whl.metadata (2.3 kB)\n",
            "Collecting langsmith<0.2.0,>=0.1.17 (from langchain)\n",
            "  Downloading langsmith-0.1.132-py3-none-any.whl.metadata (13 kB)\n",
            "Requirement already satisfied: numpy<2,>=1 in /usr/local/lib/python3.10/dist-packages (from langchain) (1.26.4)\n",
            "Requirement already satisfied: pydantic<3.0.0,>=2.7.4 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.9.2)\n",
            "Requirement already satisfied: requests<3,>=2 in /usr/local/lib/python3.10/dist-packages (from langchain) (2.32.3)\n",
            "Collecting tenacity!=8.4.0,<9.0.0,>=8.1.0 (from langchain)\n",
            "  Downloading tenacity-8.5.0-py3-none-any.whl.metadata (1.2 kB)\n",
            "Collecting openai<2.0.0,>=1.40.0 (from langchain-openai)\n",
            "  Downloading openai-1.51.2-py3-none-any.whl.metadata (24 kB)\n",
            "Collecting tiktoken<1,>=0.7 (from langchain-openai)\n",
            "  Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (6.6 kB)\n",
            "Requirement already satisfied: aiohappyeyeballs>=2.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (2.4.3)\n",
            "Requirement already satisfied: aiosignal>=1.1.2 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.3.1)\n",
            "Requirement already satisfied: attrs>=17.3.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (24.2.0)\n",
            "Requirement already satisfied: frozenlist>=1.1.1 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.4.1)\n",
            "Requirement already satisfied: multidict<7.0,>=4.5 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (6.1.0)\n",
            "Requirement already satisfied: yarl<2.0,>=1.12.0 in /usr/local/lib/python3.10/dist-packages (from aiohttp<4.0.0,>=3.8.3->langchain) (1.13.1)\n",
            "Collecting jsonpatch<2.0,>=1.33 (from langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpatch-1.33-py2.py3-none-any.whl.metadata (3.0 kB)\n",
            "Requirement already satisfied: packaging<25,>=23.2 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (24.1)\n",
            "Requirement already satisfied: typing-extensions>=4.7 in /usr/local/lib/python3.10/dist-packages (from langchain-core<0.4.0,>=0.3.10->langchain) (4.12.2)\n",
            "Collecting httpx<1,>=0.23.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpx-0.27.2-py3-none-any.whl.metadata (7.1 kB)\n",
            "Collecting orjson<4.0.0,>=3.9.14 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (50 kB)\n",
            "\u001b[2K     \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m50.4/50.4 kB\u001b[0m \u001b[31m831.4 kB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hCollecting requests-toolbelt<2.0.0,>=1.0.0 (from langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading requests_toolbelt-1.0.0-py2.py3-none-any.whl.metadata (14 kB)\n",
            "Requirement already satisfied: anyio<5,>=3.5.0 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (3.7.1)\n",
            "Requirement already satisfied: distro<2,>=1.7.0 in /usr/lib/python3/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.7.0)\n",
            "Collecting jiter<1,>=0.4.0 (from openai<2.0.0,>=1.40.0->langchain-openai)\n",
            "  Downloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl.metadata (5.2 kB)\n",
            "Requirement already satisfied: sniffio in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (1.3.1)\n",
            "Requirement already satisfied: tqdm>4 in /usr/local/lib/python3.10/dist-packages (from openai<2.0.0,>=1.40.0->langchain-openai) (4.66.5)\n",
            "Requirement already satisfied: annotated-types>=0.6.0 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (0.7.0)\n",
            "Requirement already satisfied: pydantic-core==2.23.4 in /usr/local/lib/python3.10/dist-packages (from pydantic<3.0.0,>=2.7.4->langchain) (2.23.4)\n",
            "Requirement already satisfied: charset-normalizer<4,>=2 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.3.2)\n",
            "Requirement already satisfied: idna<4,>=2.5 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (3.10)\n",
            "Requirement already satisfied: urllib3<3,>=1.21.1 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2.2.3)\n",
            "Requirement already satisfied: certifi>=2017.4.17 in /usr/local/lib/python3.10/dist-packages (from requests<3,>=2->langchain) (2024.8.30)\n",
            "Requirement already satisfied: greenlet!=0.4.17 in /usr/local/lib/python3.10/dist-packages (from SQLAlchemy<3,>=1.4->langchain) (3.1.1)\n",
            "Requirement already satisfied: regex>=2022.1.18 in /usr/local/lib/python3.10/dist-packages (from tiktoken<1,>=0.7->langchain-openai) (2024.9.11)\n",
            "Requirement already satisfied: exceptiongroup in /usr/local/lib/python3.10/dist-packages (from anyio<5,>=3.5.0->openai<2.0.0,>=1.40.0->langchain-openai) (1.2.2)\n",
            "Collecting httpcore==1.* (from httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading httpcore-1.0.6-py3-none-any.whl.metadata (21 kB)\n",
            "Collecting h11<0.15,>=0.13 (from httpcore==1.*->httpx<1,>=0.23.0->langsmith<0.2.0,>=0.1.17->langchain)\n",
            "  Downloading h11-0.14.0-py3-none-any.whl.metadata (8.2 kB)\n",
            "Collecting jsonpointer>=1.9 (from jsonpatch<2.0,>=1.33->langchain-core<0.4.0,>=0.3.10->langchain)\n",
            "  Downloading jsonpointer-3.0.0-py2.py3-none-any.whl.metadata (2.3 kB)\n",
            "Downloading langchain-0.3.3-py3-none-any.whl (1.0 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.0/1.0 MB\u001b[0m \u001b[31m19.0 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_openai-0.2.2-py3-none-any.whl (49 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m49.7/49.7 kB\u001b[0m \u001b[31m2.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_core-0.3.10-py3-none-any.whl (404 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m404.4/404.4 kB\u001b[0m \u001b[31m19.9 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading langchain_text_splitters-0.3.0-py3-none-any.whl (25 kB)\n",
            "Downloading langsmith-0.1.132-py3-none-any.whl (294 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m294.6/294.6 kB\u001b[0m \u001b[31m15.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading openai-1.51.2-py3-none-any.whl (383 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m383.7/383.7 kB\u001b[0m \u001b[31m15.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading tenacity-8.5.0-py3-none-any.whl (28 kB)\n",
            "Downloading tiktoken-0.8.0-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (1.2 MB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m1.2/1.2 MB\u001b[0m \u001b[31m30.6 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpx-0.27.2-py3-none-any.whl (76 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m76.4/76.4 kB\u001b[0m \u001b[31m3.5 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading httpcore-1.0.6-py3-none-any.whl (78 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m78.0/78.0 kB\u001b[0m \u001b[31m4.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jiter-0.6.1-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (325 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m325.2/325.2 kB\u001b[0m \u001b[31m13.7 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpatch-1.33-py2.py3-none-any.whl (12 kB)\n",
            "Downloading orjson-3.10.7-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (141 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m141.9/141.9 kB\u001b[0m \u001b[31m7.1 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading requests_toolbelt-1.0.0-py2.py3-none-any.whl (54 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m54.5/54.5 kB\u001b[0m \u001b[31m3.4 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hDownloading jsonpointer-3.0.0-py2.py3-none-any.whl (7.6 kB)\n",
            "Downloading h11-0.14.0-py3-none-any.whl (58 kB)\n",
            "\u001b[2K   \u001b[90m━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━━\u001b[0m \u001b[32m58.3/58.3 kB\u001b[0m \u001b[31m3.3 MB/s\u001b[0m eta \u001b[36m0:00:00\u001b[0m\n",
            "\u001b[?25hInstalling collected packages: tenacity, orjson, jsonpointer, jiter, h11, tiktoken, requests-toolbelt, jsonpatch, httpcore, httpx, openai, langsmith, langchain-core, langchain-text-splitters, langchain-openai, langchain\n",
            "  Attempting uninstall: tenacity\n",
            "    Found existing installation: tenacity 9.0.0\n",
            "    Uninstalling tenacity-9.0.0:\n",
            "      Successfully uninstalled tenacity-9.0.0\n",
            "Successfully installed h11-0.14.0 httpcore-1.0.6 httpx-0.27.2 jiter-0.6.1 jsonpatch-1.33 jsonpointer-3.0.0 langchain-0.3.3 langchain-core-0.3.10 langchain-openai-0.2.2 langchain-text-splitters-0.3.0 langsmith-0.1.132 openai-1.51.2 orjson-3.10.7 requests-toolbelt-1.0.0 tenacity-8.5.0 tiktoken-0.8.0\n"
          ]
        }
      ],
      "source": [
        "!pip install langchain langchain-openai"
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "import os\n",
        "# import getpass\n",
        "from google.colab import userdata\n",
        "\n",
        "\n",
        "os.environ['OPENAI_API_KEY'] = userdata.get('myopenAIKey')\n",
        "\n",
        "\n",
        "from langchain_openai import ChatOpenAI\n",
        "\n",
        "model = ChatOpenAI(model_name='gpt-4o-mini')"
      ],
      "metadata": {
        "id": "qG4qztjKq-DL"
      },
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "4LdTPwxHr1kp"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "cpqvZNyFot6c"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import HumanMessage, SystemMessage\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"You are AI assistant\"),\n",
        "    HumanMessage(content=\"How are you doing today ? Answer in 1 line only\"),\n",
        "]"
      ],
      "metadata": {
        "id": "UA2gT-QhpBrV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result = model.invoke(messages)"
      ],
      "metadata": {
        "id": "HuMs-OIspcWf"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "result"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "N2B0d_mxqXZm",
        "outputId": "c592d9bf-75d6-4fe1-9c84-7ed75f132c3f"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"I'm just a program, but I'm here and ready to help you!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 27, 'total_tokens': 41, 'completion_tokens_details': {'reasoning_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_1bb46167f9', 'finish_reason': 'stop', 'logprobs': None}, id='run-afa6c53f-fe82-4d17-a1fb-fe2d775e030d-0', usage_metadata={'input_tokens': 27, 'output_tokens': 14, 'total_tokens': 41})"
            ]
          },
          "metadata": {},
          "execution_count": 16
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.output_parsers import StrOutputParser\n",
        "\n",
        "output_parser = StrOutputParser()"
      ],
      "metadata": {
        "id": "q9TDsYqFpghh"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "output_parser.invoke(result)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "P6ulN3UkqHDU",
        "outputId": "57ba1234-fa54-4820-e6f0-f7cab76688af"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm just a program, but I'm here and ready to help you!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 18
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = model | output_parser"
      ],
      "metadata": {
        "id": "-7ZdUffkqOJH"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "7OBowj_7qeo0",
        "outputId": "80a11cbb-a11f-44aa-f8f8-3ebf53c7bc39"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "\"I'm just a program, but I'm here and ready to assist you!\""
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 20
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Prompt Template\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate\n",
        "\n",
        "system_template = \"You are an AI assistant. You have to translate the following into {language}\"\n",
        "\n",
        "prompt_template = ChatPromptTemplate.from_messages([(\"system\",system_template), (\"user\",\"{user_input}\")])\n",
        "\n",
        "result = prompt_template.invoke({\"language\":\"English\", \"user_input\":\"Hola Mundo\"})\n",
        "\n",
        "result\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "cxwmlMFpqkXw",
        "outputId": "81cc8abc-3843-4cf0-9b2d-a90ffd7d214d"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "ChatPromptValue(messages=[SystemMessage(content='You are an AI assistant. You have to translate the following into English', additional_kwargs={}, response_metadata={}), HumanMessage(content='Hola Mundo', additional_kwargs={}, response_metadata={})])"
            ]
          },
          "metadata": {},
          "execution_count": 22
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "chain = prompt_template | model | output_parser"
      ],
      "metadata": {
        "id": "zlr_hgixrWWV"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "chain.invoke({\"language\":\"English\", \"user_input\":\"Hola Mundo\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "id": "r34bxqiLrmH1",
        "outputId": "f296cbe3-29d9-4f8e-e46f-1b324aba3781"
      },
      "execution_count": null,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Hello World'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 24
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Build an inital chatbot"
      ],
      "metadata": {
        "id": "YJjK95cLF5q8"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "\n",
        "from langchain_core.chat_history import BaseChatMessageHistory, InMemoryChatMessageHistory\n",
        "\n",
        "from langchain_core.runnables.history import RunnableWithMessageHistory\n",
        "\n",
        "from langchain_core.messages import HumanMessage, AIMessage, SystemMessage\n",
        "\n",
        "## Source: https://python.langchain.com/v0.1/docs/modules/model_io/chat/message_types/\n",
        "# HumanMessage:\n",
        "# Represents a message from the user.\n",
        "# Generally consists only of content provided by the human\n",
        "\n",
        "# AIMessage:\n",
        "# Represents a message from the AI model.\n",
        "# May include additional information such as tool calls if using specific functionalities like OpenAI tool calling\n",
        "\n",
        "# SystemMessage:\n",
        "# Represents a system message that provides instructions or context to the AI model.\n",
        "# Generally consists of content that guides the AI on how to behave"
      ],
      "metadata": {
        "id": "9GOqE58SrtfX"
      },
      "execution_count": 3,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How to handle memory / chat history ? ###\n",
        "1. Use **ConversationBufferMemory**, ConversationStringBufferMemory, etc. + **ChatHistory**. It has been depreacted in v0.3 and moved to langGraph using langGraph persistence ( **Source: https://python.langchain.com/docs/versions/migrating_memory/**) . So, it is advised to use either langGraph for it or use RunnableWithMessageHistory in langChain\n",
        "2. We can also wrap the chat model within **RunnableWithMessageHistory** along with InMemoryChatMessageHistory. This is the updated technique in langchain_core v-0.3\n",
        "\n",
        "* What is ChatHistory ?\n",
        "  > It can be implemented with in-memory storage or persistent storage like Redis\n",
        "\n",
        "* Note: ConversationBufferMemory does not allow distinguishing between different users and requires implementing user session management. chat_history allows collecting chat history from the frontend and attaching it to the request\n",
        "\n",
        "### How to manage length of chat memory ?\n",
        "* One important concept to understand when building chatbots is how to manage conversation history. If left unmanaged, the list of messages will grow unbounded and potentially overflow the context window of the LLM. Therefore, it is important to add a step that limits the size of the messages you are passing in.\n",
        "* Importantly, you will want to do this BEFORE the prompt template but AFTER you load previous messages from Message History\n",
        "* When using ConversationBufferMemory, we have various options like window, summary, etc.\n",
        "* Now, we have trim and filter message for simple chatbots"
      ],
      "metadata": {
        "id": "RP4uNV_qGJYf"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### RunnableWithMessageHistory"
      ],
      "metadata": {
        "id": "L22RqwT4jSMA"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = InMemoryChatMessageHistory()\n",
        "  return store[session_id]\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(model, get_session_history)\n",
        "\n",
        "\n",
        "config = {\"configurable\" : {\"session_id\":\"session_123\"}}\n",
        "\n",
        "\n",
        "response = with_message_history.invoke([HumanMessage(content=\"Hi I am Manpreet\")], config=config)\n",
        "\n",
        "print(response)\n",
        "print(response.content)"
      ],
      "metadata": {
        "id": "vSpJEdlqF2WT",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "d7af98d0-99bc-4f9c-beda-18a74187b3c5"
      },
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "content='Hello Manpreet! How can I assist you today?' additional_kwargs={'refusal': None} response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None} id='run-0b89b2fb-1b34-4542-be8e-03acdeebe730-0' usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}\n",
            "Hello Manpreet! How can I assist you today?\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke([HumanMessage(content=\"What is my name ?\")], config=config)\n",
        "response.content"
      ],
      "metadata": {
        "id": "3fwikJmkJN49",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 35
        },
        "outputId": "e5fe3d0d-91be-469b-a461-575af04e0c8d"
      },
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "'Your name is Manpreet. How can I help you today?'"
            ],
            "application/vnd.google.colaboratory.intrinsic+json": {
              "type": "string"
            }
          },
          "metadata": {},
          "execution_count": 12
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "K8cfnalMJaZO",
        "outputId": "719ac478-3066-464e-d47f-082ba94b9193"
      },
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'session_123': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manpreet', additional_kwargs={}, response_metadata={}), AIMessage(content='Hello Manpreet! How can I assist you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 12, 'prompt_tokens': 13, 'total_tokens': 25, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-0b89b2fb-1b34-4542-be8e-03acdeebe730-0', usage_metadata={'input_tokens': 13, 'output_tokens': 12, 'total_tokens': 25, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='What is my name ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your name is Manpreet. How can I help you today?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 14, 'prompt_tokens': 38, 'total_tokens': 52, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-316da9b2-b63c-402b-b8c5-c4009f6ea50e-0', usage_metadata={'input_tokens': 38, 'output_tokens': 14, 'total_tokens': 52, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 13
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## we can always go back to the original conversation by pointing to the sessionid\n"
      ],
      "metadata": {
        "id": "9JEGAAGJJgzl"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task1: Store the session details. It will store the entire conversation to the current chat time\n",
        "### Here, we use InMemoryChatMessageHistory, which is an alternative to ConversationBufferMemory\n",
        " #### (which was deprecated in langchain and moved to to langGraph)\n",
        " #### https://python.langchain.com/docs/versions/migrating_memory/conversation_buffer_memory/\n",
        "\n",
        "store = {}\n",
        "\n",
        "def get_session_history(session_id: str):\n",
        "  if session_id not in store:\n",
        "    store[session_id] = InMemoryChatMessageHistory()\n",
        "  return store[session_id]"
      ],
      "metadata": {
        "id": "poOCxzcRdVca"
      },
      "execution_count": 112,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task2: Define the prompt and add information of System, Human, and chatHistory\n",
        "\n",
        "from langchain_core.prompts import ChatPromptTemplate, MessagesPlaceholder\n",
        "\n",
        "system_mesg = \"You are a helpful assistant. Your job is to respond to my queries like my mentor and an as life coach in {my_tone} tone\"\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\"system\", system_mesg),\n",
        "        MessagesPlaceholder(variable_name=\"messages\"),\n",
        "    ]\n",
        ")\n"
      ],
      "metadata": {
        "id": "a3Runa9aOVzh"
      },
      "execution_count": 113,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task3: Chain them:\n",
        "#### Ealrier LLMChain was used but now with Runnables, we have LCEL (LangChain Execution Layer), which makes it very simple to chain the Runnables\n",
        "##### together with '|' symbol in the order of execution\n",
        "\n",
        "### Legacy / older code using LLmChain\n",
        "# from langchain.chains import LLMChain\n",
        "# from langchain_core.prompts import ChatPromptTemplate\n",
        "# from langchain_openai import ChatOpenAI\n",
        "\n",
        "# prompt = ChatPromptTemplate.from_messages(\n",
        "#     [(\"user\", \"Tell me a {adjective} joke\")],\n",
        "# )\n",
        "\n",
        "# legacy_chain = LLMChain(llm=ChatOpenAI(), prompt=prompt)\n",
        "\n",
        "\n",
        "\n",
        "### below is the new code:\n",
        "\n",
        "\n",
        "chain = prompt | model\n"
      ],
      "metadata": {
        "id": "11EmSSArel_L"
      },
      "execution_count": 114,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### old way to invoke:\n",
        "# legacy_result = legacy_chain({\"adjective\": \"funny\"})\n",
        "# legacy_result\n",
        "\n",
        "\n",
        "### Invoking the new chains:\n",
        "\n",
        "chain.invoke({\"messages\":[HumanMessage(content=\"Hi I am manpreet. How are you ?\")],\n",
        "              \"my_tone\":\"friendly\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "6TpYvOp18XTZ",
        "outputId": "00340aaa-e3f0-4301-b5ff-7b28e37c6842"
      },
      "execution_count": 115,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 47, 'total_tokens': 72, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-945a5dd4-1c42-4e21-9a20-8934048ce3bd-0', usage_metadata={'input_tokens': 47, 'output_tokens': 25, 'total_tokens': 72, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 115
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### There is no memeory or historical chain involved here.\n",
        "chain.invoke({\"messages\":[HumanMessage(content=\"Can you tell my name ?\")],\n",
        "              \"my_tone\":\"aggression\"})"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ORKuf3OvxVEE",
        "outputId": "18adf59c-ea59-4af8-866d-cf5bf430e165"
      },
      "execution_count": 116,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Listen up! I don’t have the ability to know your name unless you tell me. So, what’s your name? Let’s get this conversation rolling!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 32, 'prompt_tokens': 42, 'total_tokens': 74, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-8b5c9839-601e-4408-812a-fc90d231fca4-0', usage_metadata={'input_tokens': 42, 'output_tokens': 32, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 116
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### Task4: Add user History (memory) to User Input to make a proper bot conversation\n",
        "#### We added stores to store the History, now let's see how to use that history and pass it in the prompt along with user query\n",
        "##### Earlier we used to use ConversationChain, but now it has been replaced with RunnableWithMessageHistory\n",
        "\n",
        "\n",
        "##### New Method using RunnableWithMessageHistory\n",
        "\n",
        "with_message_history = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")\n",
        "\n",
        "\n"
      ],
      "metadata": {
        "id": "r5zNGccT6DQ5"
      },
      "execution_count": 117,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task5: Setup the session_id which will load the historical chatg and add it the current conversation\n",
        "#### config has a fixed style and it expects configurable and session_id keywords\n",
        "config = {\"configurable\": {\"session_id\": \"abc11\"}}\n",
        "\n"
      ],
      "metadata": {
        "id": "O83S1jzutSfH"
      },
      "execution_count": 118,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Task6: Chat\n",
        "\n",
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi I am Manpreet. How are you doing ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "9z68G3V4tT7h",
        "outputId": "b29eccb7-8f14-45c3-ef55-843f4e9a560d"
      },
      "execution_count": 119,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 48, 'total_tokens': 73, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-0baf051e-f94e-4aa3-9673-fa118def3bfe-0', usage_metadata={'input_tokens': 48, 'output_tokens': 25, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 119
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\" I am 31 years old and I like to sketch\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "ZyIuKGCR96JX",
        "outputId": "8f307216-9e6b-4b1a-daca-22e4195c0494"
      },
      "execution_count": 120,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 92, 'total_tokens': 143, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99f3790-e2da-4db0-a0c5-f7e38bac4f52-0', usage_metadata={'input_tokens': 92, 'output_tokens': 51, 'total_tokens': 143, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 120
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### It will be able to answer because we have kept the memory by using RunnableWithMessageHistory.\n",
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\" What do you think is my hobby ?\")],\n",
        "                                        \"my_tone\":\"aggressive\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Uw8bzQ6V0AfT",
        "outputId": "fad15be1-b117-4d0c-801d-977e98d279b1"
      },
      "execution_count": 121,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 159, 'total_tokens': 251, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-12d21408-40b2-43c5-90a8-382c29bfd344-0', usage_metadata={'input_tokens': 159, 'output_tokens': 92, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 121
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## This will store the entire conversation w.r.t each sessionId\n",
        "\n",
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "wKTU6esO-ENR",
        "outputId": "c515e3e3-148f-4f11-9c60-58d13a39db47"
      },
      "execution_count": 122,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abc11': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manpreet. How are you doing ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 48, 'total_tokens': 73, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-0baf051e-f94e-4aa3-9673-fa118def3bfe-0', usage_metadata={'input_tokens': 48, 'output_tokens': 25, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}), AIMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 92, 'total_tokens': 143, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99f3790-e2da-4db0-a0c5-f7e38bac4f52-0', usage_metadata={'input_tokens': 92, 'output_tokens': 51, 'total_tokens': 143, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 159, 'total_tokens': 251, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-12d21408-40b2-43c5-90a8-382c29bfd344-0', usage_metadata={'input_tokens': 159, 'output_tokens': 92, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 122
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store['abc11']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "yM2_PBuW0ILz",
        "outputId": "f0dc75ff-c94e-41f4-b235-78ff508456dc"
      },
      "execution_count": 123,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manpreet. How are you doing ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 48, 'total_tokens': 73, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-0baf051e-f94e-4aa3-9673-fa118def3bfe-0', usage_metadata={'input_tokens': 48, 'output_tokens': 25, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}), AIMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 92, 'total_tokens': 143, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99f3790-e2da-4db0-a0c5-f7e38bac4f52-0', usage_metadata={'input_tokens': 92, 'output_tokens': 51, 'total_tokens': 143, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 159, 'total_tokens': 251, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-12d21408-40b2-43c5-90a8-382c29bfd344-0', usage_metadata={'input_tokens': 159, 'output_tokens': 92, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])"
            ]
          },
          "metadata": {},
          "execution_count": 123
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "## NOTE: We can always activate the bot in a conversation way and load the historical conversation just by mapping user_id with session_id\n",
        "## and when that user will login, we will pass that user's id and fetch his/her sessionId and use that to resume the chat with history"
      ],
      "metadata": {
        "id": "sEnXC8RxsrnS"
      },
      "execution_count": 124,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "# prompt: get all HumanMessage from store\n",
        "\n",
        "message_conv = [message.content for message in store['abc11'].messages if isinstance(message, HumanMessage) or isinstance(message, AIMessage) ]\n"
      ],
      "metadata": {
        "id": "PW8vFLhK0o7Z"
      },
      "execution_count": 125,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "for message in store['abc11'].messages:\n",
        "  if isinstance(message, HumanMessage):\n",
        "    print(f\"Human: {message.content}\")\n",
        "  elif isinstance(message, AIMessage):\n",
        "    print(f\"AI: {message.content}\")"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "EkEdJvce1vej",
        "outputId": "ff48654d-a1af-4487-8916-1927e9aed1cc"
      },
      "execution_count": 126,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Human: Hi I am Manpreet. How are you doing ?\n",
            "AI: Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\n",
            "Human:  I am 31 years old and I like to sketch\n",
            "AI: That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?\n",
            "Human:  What do you think is my hobby ?\n",
            "AI: Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "wC1p4eyx3PNl"
      },
      "execution_count": 126,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Add trim_message functionality to save chat from exploding and keeping only latest K messages\n"
      ],
      "metadata": {
        "id": "rc3RtlTM3Vqn"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=200,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")"
      ],
      "metadata": {
        "id": "7hKSHgUp-JoJ"
      },
      "execution_count": 127,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer.invoke(store['abc11'].messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UwfnArPp4CTW",
        "outputId": "dc59a64e-f1e1-4e70-cec6-bcd8343a6600",
        "collapsed": true
      },
      "execution_count": 128,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 92, 'total_tokens': 143, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99f3790-e2da-4db0-a0c5-f7e38bac4f52-0', usage_metadata={'input_tokens': 92, 'output_tokens': 51, 'total_tokens': 143, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              " HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 159, 'total_tokens': 251, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-12d21408-40b2-43c5-90a8-382c29bfd344-0', usage_metadata={'input_tokens': 159, 'output_tokens': 92, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 128
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### This gives more context as we pull the messages only and delete additional information\n",
        "trimmer.invoke(message_conv)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "B6hCinGWEt7k",
        "outputId": "d849da93-6c5e-4055-a293-72de2996529a"
      },
      "execution_count": 129,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 129
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "Tkde_VasG-eF",
        "outputId": "d9974e60-79ab-4541-f1af-57091dc8be5f"
      },
      "execution_count": 130,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableLambda(...)"
            ]
          },
          "metadata": {},
          "execution_count": 130
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough"
      ],
      "metadata": {
        "id": "d6zNzzG09yPp"
      },
      "execution_count": 131,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "new_chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"messages\") | trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        ")"
      ],
      "metadata": {
        "id": "BlF9Mlz-4RcC"
      },
      "execution_count": 132,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### No Memory\n",
        "new_chain.invoke({\"messages\": [HumanMessage(content=\"what is my hobby and name ?\")],\n",
        "              \"my_tone\":\"soft\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "JZoC-Z_d0lHv",
        "outputId": "cc67df19-afe9-4e4b-f721-cf587bd1bb52"
      },
      "execution_count": 134,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"That's a great question! While I don’t have access to your personal details, I can help you explore what your hobbies might be. Think about activities that make you feel happy and fulfilled. Do you enjoy painting, reading, playing sports, or perhaps gardening? \\n\\nAs for your name, that's something only you can share! If you'd like to tell me, I'm here to listen. Remember, discovering your interests and passions is a wonderful journey, and I'm here to support you every step of the way.\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 101, 'prompt_tokens': 43, 'total_tokens': 144, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a13949f-a358-42c3-ad60-13a3f4a70a30-0', usage_metadata={'input_tokens': 43, 'output_tokens': 101, 'total_tokens': 144, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 134
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "### With Memory\n",
        "new_chain.invoke({\"messages\": message_conv + [HumanMessage(content=\"what is my hobby and name ?\")],\n",
        "              \"my_tone\":\"soft\"})\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "J2zsD3NpK1Ly",
        "outputId": "f023d031-0b6e-4666-dfdb-236a4ae0dbf1"
      },
      "execution_count": 133,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Your hobby is sketching, and your name is Manpreet. It's lovely to see you embracing your creative side! If you have any specific goals or aspirations related to your sketching, I'd love to hear about them. Whether it's improving your technique, exploring new styles, or even sharing your art with others, there are so many exciting paths you can take. How can I support you in your journey?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 82, 'prompt_tokens': 221, 'total_tokens': 303, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-6ea6af6d-d283-40b2-b48f-7fa600c62d0e-0', usage_metadata={'input_tokens': 221, 'output_tokens': 82, 'total_tokens': 303, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 133
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "# new_chain.invoke({\"messages\": trimmer.invoke(message_conv) + [HumanMessage(content=\"what is my hobby and name ?\")],\n",
        "#               \"my_tone\":\"soft\"})\n",
        "\n",
        "### We can trimmer in the chain.invoke as well and remove where chain is getting declaared"
      ],
      "metadata": {
        "id": "Oj2x3I591IN5"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "### Add trimmer to RunnableMessageHistory"
      ],
      "metadata": {
        "id": "PBV9eiE90woc"
      },
      "execution_count": 135,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history = RunnableWithMessageHistory(\n",
        "    new_chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")"
      ],
      "metadata": {
        "id": "VfSoaD8W94rp"
      },
      "execution_count": 136,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "lT65Ts5EDh8W",
        "outputId": "79fe5da1-e731-4128-9067-c2e1957e22e4"
      },
      "execution_count": 137,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "RunnableWithMessageHistory(bound=RunnableBinding(bound=RunnableBinding(bound=RunnableAssign(mapper={\n",
              "  messages: RunnableBinding(bound=RunnableLambda(_enter_history), kwargs={}, config={'run_name': 'load_history'}, config_factories=[])\n",
              "}), kwargs={}, config={'run_name': 'insert_history'}, config_factories=[])\n",
              "| RunnableBinding(bound=RunnableLambda(_call_runnable_sync), kwargs={}, config={'run_name': 'check_sync_or_async'}, config_factories=[]), kwargs={}, config={'run_name': 'RunnableWithMessageHistory'}, config_factories=[]), kwargs={}, config={}, config_factories=[], get_session_history=<function get_session_history at 0x79bc88f25900>, input_messages_key='messages', history_factory_config=[ConfigurableFieldSpec(id='session_id', annotation=<class 'str'>, name='Session ID', description='Unique identifier for a session.', default='', is_shared=True, dependencies=None)])"
            ]
          },
          "metadata": {},
          "execution_count": 137
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"Hi I am Manpreet. How are you doing ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "rZrWLX6F1VCF",
        "outputId": "25e3804d-b641-4d03-bdeb-4fe040c2fc81"
      },
      "execution_count": 139,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Hi Manpreet! I’m doing well, thank you! I’m really glad to be here, ready to chat with you. How about you? How’s your sketching going? Any new projects or ideas you’re excited about?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 226, 'total_tokens': 274, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5466ef8-b24a-4ca4-b6c3-d1a4c9ee5945-0', usage_metadata={'input_tokens': 226, 'output_tokens': 48, 'total_tokens': 274, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 139
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "collapsed": true,
        "id": "JKaVUvpJ1y5o",
        "outputId": "25149701-b048-452f-8e84-85136252ce3a"
      },
      "execution_count": 140,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abc11': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manpreet. How are you doing ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 25, 'prompt_tokens': 48, 'total_tokens': 73, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-0baf051e-f94e-4aa3-9673-fa118def3bfe-0', usage_metadata={'input_tokens': 48, 'output_tokens': 25, 'total_tokens': 73, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}), AIMessage(content='That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 51, 'prompt_tokens': 92, 'total_tokens': 143, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-e99f3790-e2da-4db0-a0c5-f7e38bac4f52-0', usage_metadata={'input_tokens': 92, 'output_tokens': 51, 'total_tokens': 143, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 92, 'prompt_tokens': 159, 'total_tokens': 251, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-12d21408-40b2-43c5-90a8-382c29bfd344-0', usage_metadata={'input_tokens': 159, 'output_tokens': 92, 'total_tokens': 251, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content='Hi I am Manpreet. How are you doing ?', additional_kwargs={}, response_metadata={}), AIMessage(content='Hi Manpreet! I’m doing well, thank you! I’m really glad to be here, ready to chat with you. How about you? How’s your sketching going? Any new projects or ideas you’re excited about?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 48, 'prompt_tokens': 226, 'total_tokens': 274, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-f5466ef8-b24a-4ca4-b6c3-d1a4c9ee5945-0', usage_metadata={'input_tokens': 226, 'output_tokens': 48, 'total_tokens': 274, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 140
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What is 2*4 + 2 ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UBSnSaVw17-k",
        "outputId": "afeae6f9-6a01-4844-dd5c-36eb61b195ee"
      },
      "execution_count": 141,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='The calculation is quite simple! First, you multiply 2 by 4, which gives you 8. Then you add 2 to that result. So, 2 * 4 + 2 = 8 + 2 = 10. If you have any more questions or need help with anything else, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 222, 'total_tokens': 291, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea25870c-7043-4aa3-a0e5-1bf2039c4ce3-0', usage_metadata={'input_tokens': 222, 'output_tokens': 69, 'total_tokens': 291, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 141
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What is the colour of carbon monoxide ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "92ZfTw1i1_VZ",
        "outputId": "d324a339-c4de-4534-fedd-33b2edeedbdf"
      },
      "execution_count": 142,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content=\"Carbon monoxide (CO) is actually colorless, odorless, and tasteless. This makes it particularly dangerous because you can't see or smell it, which is why it's important to have carbon monoxide detectors in your home if you use gas appliances or have a garage attached to your house. If you have more questions about safety or anything else, I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 199, 'total_tokens': 272, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a271533-5626-4e29-8bae-5a66e925fa5b-0', usage_metadata={'input_tokens': 199, 'output_tokens': 73, 'total_tokens': 272, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 142
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "trimmer.invoke(store['abc11'].messages)\n",
        "### here the trimmer has trimmed old chats regarding my name"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "aEBNfaTMJWHI",
        "outputId": "b971cb8e-a755-475a-ce29-9c962d77e408",
        "collapsed": true
      },
      "execution_count": 146,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[HumanMessage(content='What is 2*4 + 2 ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='The calculation is quite simple! First, you multiply 2 by 4, which gives you 8. Then you add 2 to that result. So, 2 * 4 + 2 = 8 + 2 = 10. If you have any more questions or need help with anything else, feel free to ask!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 69, 'prompt_tokens': 222, 'total_tokens': 291, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-ea25870c-7043-4aa3-a0e5-1bf2039c4ce3-0', usage_metadata={'input_tokens': 222, 'output_tokens': 69, 'total_tokens': 291, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}),\n",
              " HumanMessage(content='What is the colour of carbon monoxide ?', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content=\"Carbon monoxide (CO) is actually colorless, odorless, and tasteless. This makes it particularly dangerous because you can't see or smell it, which is why it's important to have carbon monoxide detectors in your home if you use gas appliances or have a garage attached to your house. If you have more questions about safety or anything else, I'm here to help!\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 73, 'prompt_tokens': 199, 'total_tokens': 272, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-6a271533-5626-4e29-8bae-5a66e925fa5b-0', usage_metadata={'input_tokens': 199, 'output_tokens': 73, 'total_tokens': 272, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})]"
            ]
          },
          "metadata": {},
          "execution_count": 146
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What is my name ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response\n",
        "\n",
        "## Now, it is not able to tell the name because trimmer has trimmed the earlier messages"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "iHBSOjTl2V7i",
        "outputId": "b7bdf315-bfa3-4393-da9d-b57154869528"
      },
      "execution_count": 147,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='I actually don’t know your name unless you tell me! But I’d love to know it if you’d like to share. What’s your name?', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 31, 'prompt_tokens': 217, 'total_tokens': 248, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-4b3512c9-01d4-4f17-a302-3ae54263fea5-0', usage_metadata={'input_tokens': 217, 'output_tokens': 31, 'total_tokens': 248, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 147
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "with_message_history_oldchain = RunnableWithMessageHistory(\n",
        "    chain,\n",
        "    get_session_history,\n",
        "    input_messages_key=\"messages\",\n",
        ")"
      ],
      "metadata": {
        "id": "XnOnGGdK5Y33"
      },
      "execution_count": 148,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "response = with_message_history_oldchain.invoke(\n",
        "    {\"messages\": [HumanMessage(content=\"What is my name ?\")],\n",
        "                                        \"my_tone\":\"friendly\"},\n",
        "                                         config=config)\n",
        "response\n",
        "\n",
        "## Now, it is able to tell the name because no trimmer has been activated as we have used old chain"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "_0oW4R8_5dFz",
        "outputId": "9c11bf91-c91d-4d0c-9d5b-658020dd828a"
      },
      "execution_count": 150,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "AIMessage(content='Your name is Manpreet! It’s great to chat with you, Manpreet. If there’s anything specific you’d like to discuss or ask about, just let me know!', additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 38, 'prompt_tokens': 612, 'total_tokens': 650, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-52b3a049-6616-45a2-b704-9cd03d6a5135-0', usage_metadata={'input_tokens': 612, 'output_tokens': 38, 'total_tokens': 650, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})"
            ]
          },
          "metadata": {},
          "execution_count": 150
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "WceLSsS5IjAU"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "AZkYGjj4Ftn9"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "## Archive"
      ],
      "metadata": {
        "id": "VoSpnr14FuFV"
      }
    },
    {
      "cell_type": "markdown",
      "source": [
        "### Quick observation of why not use any custom key=\"user_message\" but use \"messagess only"
      ],
      "metadata": {
        "id": "dTma4k6Q5sj_"
      }
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "##\n",
        "def get_and_trim_user_messages(data, trimmer, key=\"user_messages\"):\n",
        "    messages = itemgetter(key)(data)\n",
        "    return trimmer(messages)\n",
        "\n",
        "def trimmer(messages, max_length=5):\n",
        "    return messages[:max_length]\n",
        "\n",
        "runnable = RunnablePassthrough.assign(\n",
        "    messages=lambda data: get_and_trim_user_messages(data, trimmer)\n",
        ")\n",
        "\n",
        "\n",
        "# Example input\n",
        "input_data = {\n",
        "    \"user_messages\": [\"msg1\", \"msg2\", \"msg3\", \"msg4\", \"msg5\", \"msg6\", \"msg7\", \"msg8\", \"msg9\", \"msg10\", \"msg11\"]\n",
        "}\n",
        "\n",
        "# Invoke the runnable\n",
        "output = runnable.invoke(input_data)\n",
        "\n",
        "print(output)\n",
        "# Output: {'user_messages': ['msg1', 'msg2', 'msg3', 'msg4', 'msg5', 'msg6', 'msg7', 'msg8', 'msg9', 'msg10', 'msg11'], 'messages': ['msg1', 'msg2', 'msg3', 'msg4', 'msg5', 'msg6', 'msg7', 'msg8', 'msg9', 'msg10']}\n"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "MxIQ00wvD_eO",
        "outputId": "77ab6545-cbaa-48c7-b67b-b406b2cb30ea"
      },
      "execution_count": 175,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "{'user_messages': ['msg1', 'msg2', 'msg3', 'msg4', 'msg5', 'msg6', 'msg7', 'msg8', 'msg9', 'msg10', 'msg11'], 'messages': ['msg1', 'msg2', 'msg3', 'msg4', 'msg5']}\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "output['messages']"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "05HehcXtMPQ3",
        "outputId": "84069f0f-ed11-4db4-c9c8-a12a27d8e21a"
      },
      "execution_count": 174,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['msg1', 'msg2', 'msg3', 'msg4', 'msg5']"
            ]
          },
          "metadata": {},
          "execution_count": 174
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Gyjla0Dsugx_"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from langchain_core.messages import SystemMessage, trim_messages\n",
        "\n",
        "trimmer = trim_messages(\n",
        "    max_tokens=50,\n",
        "    strategy=\"last\",\n",
        "    token_counter=model,\n",
        "    include_system=True,\n",
        "    allow_partial=False,\n",
        "    start_on=\"human\",\n",
        ")\n",
        "\n",
        "messages = [\n",
        "    SystemMessage(content=\"you're a good assistant\"),\n",
        "    HumanMessage(content=\"hi! I'm bob\"),\n",
        "    AIMessage(content=\"hi!\"),\n",
        "    HumanMessage(content=\"I like vanilla ice cream\"),\n",
        "    AIMessage(content=\"nice\"),\n",
        "    HumanMessage(content=\"whats 2 + 2\"),\n",
        "    AIMessage(content=\"4\"),\n",
        "    HumanMessage(content=\"thanks\"),\n",
        "    AIMessage(content=\"no problem!\"),\n",
        "    HumanMessage(content=\"my hobby is art\"),\n",
        "    AIMessage(content=\"great!\"),\n",
        "]\n",
        "\n",
        "trimmer.invoke(messages)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "UCtktYD-PiHm",
        "outputId": "5669cac7-be31-494e-afb6-7c324461a66f"
      },
      "execution_count": 170,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "[SystemMessage(content=\"you're a good assistant\", additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='thanks', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='no problem!', additional_kwargs={}, response_metadata={}),\n",
              " HumanMessage(content='my hobby is art', additional_kwargs={}, response_metadata={}),\n",
              " AIMessage(content='great!', additional_kwargs={}, response_metadata={})]"
            ]
          },
          "metadata": {},
          "execution_count": 170
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "GW-QEnVN9odj"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "from operator import itemgetter\n",
        "\n",
        "from langchain_core.runnables import RunnablePassthrough\n",
        "\n",
        "prompt = ChatPromptTemplate.from_messages(\n",
        "    [\n",
        "        (\n",
        "            \"system\",\n",
        "            \"You are a helpful assistant. Answer all questions to the best of your ability in {language}.\",\n",
        "        ),\n",
        "        MessagesPlaceholder(variable_name=\"user_messages\"),\n",
        "    ]\n",
        ")\n",
        "\n",
        "chain = (\n",
        "    RunnablePassthrough.assign(messages=itemgetter(\"user_messages\") | trimmer)\n",
        "    | prompt\n",
        "    | model\n",
        ")\n",
        "\n",
        "response = chain.invoke(\n",
        "    {\n",
        "        \"user_messages\": trimmer.invoke(messages) + [HumanMessage(content=\"Who am i ?\")],\n",
        "        \"language\": \"English\",\n",
        "    }\n",
        ")\n",
        "response.content"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 356
        },
        "id": "Id4ow-RsQMEM",
        "outputId": "80510ecc-e486-4411-aed8-0362082bdd72"
      },
      "execution_count": 172,
      "outputs": [
        {
          "output_type": "error",
          "ename": "AssertionError",
          "evalue": "The input to RunnablePassthrough.assign() must be a dict.",
          "traceback": [
            "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
            "\u001b[0;31mAssertionError\u001b[0m                            Traceback (most recent call last)",
            "\u001b[0;32m<ipython-input-172-9497bf3535ab>\u001b[0m in \u001b[0;36m<cell line: 21>\u001b[0;34m()\u001b[0m\n\u001b[1;32m     19\u001b[0m )\n\u001b[1;32m     20\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 21\u001b[0;31m response = chain.invoke(\n\u001b[0m\u001b[1;32m     22\u001b[0m     {\n\u001b[1;32m     23\u001b[0m         \u001b[0;34m\"user_messages\"\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mtrimmer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmessages\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mHumanMessage\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mcontent\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0;34m\"Who am i ?\"\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m   3022\u001b[0m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3023\u001b[0m                 \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 3024\u001b[0;31m                     \u001b[0minput\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcontext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mrun\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstep\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0minvoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   3025\u001b[0m         \u001b[0;31m# finish the root run\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   3026\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0mBaseException\u001b[0m \u001b[0;32mas\u001b[0m \u001b[0me\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36minvoke\u001b[0;34m(self, input, config, **kwargs)\u001b[0m\n\u001b[1;32m    492\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    493\u001b[0m     ) -> dict[str, Any]:\n\u001b[0;32m--> 494\u001b[0;31m         \u001b[0;32mreturn\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_call_with_config\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0m_invoke\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mconfig\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    495\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    496\u001b[0m     async def _ainvoke(\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/base.py\u001b[0m in \u001b[0;36m_call_with_config\u001b[0;34m(self, func, input, config, run_type, serialized, **kwargs)\u001b[0m\n\u001b[1;32m   1925\u001b[0m             output = cast(\n\u001b[1;32m   1926\u001b[0m                 \u001b[0mOutput\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1927\u001b[0;31m                 context.run(\n\u001b[0m\u001b[1;32m   1928\u001b[0m                     \u001b[0mcall_func_with_variable_args\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1929\u001b[0m                     \u001b[0mfunc\u001b[0m\u001b[0;34m,\u001b[0m  \u001b[0;31m# type: ignore[arg-type]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/config.py\u001b[0m in \u001b[0;36mcall_func_with_variable_args\u001b[0;34m(func, input, config, run_manager, **kwargs)\u001b[0m\n\u001b[1;32m    394\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mrun_manager\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m \u001b[0;32mand\u001b[0m \u001b[0maccepts_run_manager\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfunc\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    395\u001b[0m         \u001b[0mkwargs\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;34m\"run_manager\"\u001b[0m\u001b[0;34m]\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mrun_manager\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 396\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mfunc\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m)\u001b[0m  \u001b[0;31m# type: ignore[call-arg]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    397\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    398\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
            "\u001b[0;32m/usr/local/lib/python3.10/dist-packages/langchain_core/runnables/passthrough.py\u001b[0m in \u001b[0;36m_invoke\u001b[0;34m(self, input, run_manager, config, **kwargs)\u001b[0m\n\u001b[1;32m    473\u001b[0m         \u001b[0;34m**\u001b[0m\u001b[0mkwargs\u001b[0m\u001b[0;34m:\u001b[0m \u001b[0mAny\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    474\u001b[0m     ) -> dict[str, Any]:\n\u001b[0;32m--> 475\u001b[0;31m         assert isinstance(\n\u001b[0m\u001b[1;32m    476\u001b[0m             \u001b[0minput\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mdict\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    477\u001b[0m         ), \"The input to RunnablePassthrough.assign() must be a dict.\"\n",
            "\u001b[0;31mAssertionError\u001b[0m: The input to RunnablePassthrough.assign() must be a dict."
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "l_kRn1dIxhmJ",
        "outputId": "bf2ec8a4-e3f3-4f99-ca4b-d55527a0c5aa"
      },
      "execution_count": 167,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi I am Manpreet. How are you doing ?',\n",
              " \"Hi Manpreet! I'm doing well, thank you for asking. How about you? What’s on your mind today?\",\n",
              " ' I am 31 years old and I like to sketch',\n",
              " 'That’s wonderful, Manpreet! Sketching is a beautiful form of expression. It allows you to capture your thoughts and feelings in a unique way. How long have you been sketching? And what kind of subjects do you enjoy drawing the most?',\n",
              " ' What do you think is my hobby ?',\n",
              " 'Your hobby is clearly sketching! But let’s not just stop there. If you’re passionate about it, you need to dive deeper. Your hobby should not just be a pastime; it should be something that you invest your time in and develop your skills. Are you pushing yourself to improve? Are you experimenting with different styles or subjects? If not, it’s time to step up your game! What are you waiting for? Get out there and create!']"
            ]
          },
          "metadata": {},
          "execution_count": 167
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "store"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "SCi6A7Asxmhv",
        "outputId": "0f26a4da-5253-4083-f135-f8dea274a3b6"
      },
      "execution_count": 104,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "{'abc11': InMemoryChatMessageHistory(messages=[HumanMessage(content='Hi I am Manpreet. How are you doing ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Hi Manpreet! I'm doing well, thank you for asking. How about you? How's everything going in your world?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 26, 'prompt_tokens': 48, 'total_tokens': 74, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-50471c15-026e-45f5-93e2-48401489e83e-0', usage_metadata={'input_tokens': 48, 'output_tokens': 26, 'total_tokens': 74, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' I am 31 years old and I like to sketch', additional_kwargs={}, response_metadata={}), AIMessage(content=\"That's wonderful, Manpreet! Sketching is such a fantastic way to express creativity and capture the world around you. What do you enjoy sketching the most? People, landscapes, or something else?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 41, 'prompt_tokens': 93, 'total_tokens': 134, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-d875cc2f-a47b-432b-a2f5-500bef20d468-0', usage_metadata={'input_tokens': 93, 'output_tokens': 41, 'total_tokens': 134, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}}), HumanMessage(content=' What do you think is my hobby ?', additional_kwargs={}, response_metadata={}), AIMessage(content=\"Your hobby is clearly sketching, and that's great! But let me tell you, just liking to sketch isn't enough. You need to dive deep, push your limits, and really make it a part of your life. Are you sketching regularly, or are you just dabbling? Get serious about it! Turn that hobby into a passion, and start creating masterpieces! What’s stopping you from taking it to the next level?\", additional_kwargs={'refusal': None}, response_metadata={'token_usage': {'completion_tokens': 86, 'prompt_tokens': 150, 'total_tokens': 236, 'completion_tokens_details': {'audio_tokens': None, 'reasoning_tokens': 0}, 'prompt_tokens_details': {'audio_tokens': None, 'cached_tokens': 0}}, 'model_name': 'gpt-4o-mini-2024-07-18', 'system_fingerprint': 'fp_f85bea6784', 'finish_reason': 'stop', 'logprobs': None}, id='run-770b5a62-c8ce-4ad9-9f50-476b17ad63c9-0', usage_metadata={'input_tokens': 150, 'output_tokens': 86, 'total_tokens': 236, 'input_token_details': {'cache_read': 0}, 'output_token_details': {'reasoning': 0}})])}"
            ]
          },
          "metadata": {},
          "execution_count": 104
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "message_conv"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "NHLaVZbuyETy",
        "outputId": "647f7107-4244-49c9-b7b3-c28fbbb4f408"
      },
      "execution_count": 84,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "['Hi I am Manpreet. How are you doing ?',\n",
              " \"Hi Manpreet! I'm doing well, thank you for asking. How about you? How's everything going in your world?\",\n",
              " ' I am 31 years old and I like to sketch',\n",
              " \"That's wonderful, Manpreet! Sketching is such a fantastic way to express creativity and capture the world around you. What do you enjoy sketching the most? People, landscapes, or something else?\",\n",
              " ' What do you think is my hobby ?',\n",
              " \"Your hobby is clearly sketching, and that's great! But let me tell you, just liking to sketch isn't enough. You need to dive deep, push your limits, and really make it a part of your life. Are you sketching regularly, or are you just dabbling? Get serious about it! Turn that hobby into a passion, and start creating masterpieces! What’s stopping you from taking it to the next level?\"]"
            ]
          },
          "metadata": {},
          "execution_count": 84
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "NOTE:\n",
        "\n",
        "### So the trick is to give \"messages\" across everywhere and not use any custom name like \"user_messagees\".\n",
        "### If you use, RunnablePassthrough and trimmer won't work the way they are designed\n",
        "\n",
        "\n",
        "Now, correct in the real example in the notebooka and fix the code i.e update the ChatPrompt to have \"messages\" keyword only and not \"user_messages\""
      ],
      "metadata": {
        "id": "veaaqST9QQex"
      },
      "execution_count": null,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "### How can we use advanced functionalities of ConversationBuffer like window, summary along with RunnableWithMessageHistory ?\n",
        "#### **Answer: Build a custom class for it**"
      ],
      "metadata": {
        "id": "oBN8h-J0vXag"
      }
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "2iFV208hvlRv"
      },
      "execution_count": null,
      "outputs": []
    }
  ]
}