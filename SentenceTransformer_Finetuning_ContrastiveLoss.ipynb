{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Sentence Transformer Training\n",
    "from sentence_transformers import SentenceTransformer, models, util, losses, datasets\n",
    "from sentence_transformers.readers import InputExample\n",
    "from transformers import BertTokenizer, BertForMaskedLM, AutoModel, AutoTokenizer\n",
    "from torch.utils.data import DataLoader\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "# import datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "import azureml\n",
    "from azureml.core import Workspace, Datastore, Dataset\n",
    "from azureml.data.data_reference import DataReference\n",
    "import pandas as pd\n",
    "import tempfile\n",
    "import torch\n",
    "import sentence_transformers\n",
    "from sentence_transformers import SentenceTransformer,CrossEncoder, util\n",
    "\n",
    "import numpy as np\n",
    "import pickle\n",
    "import time\n",
    "import joblib"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "from azure.ai.ml.constants import AssetTypes \n",
    "from azure.ai.ml import MLClient\n",
    "from azure.ai.ml.entities import (\n",
    "    ManagedOnlineEndpoint,\n",
    "    ManagedOnlineDeployment,\n",
    "    Model,\n",
    "    Environment,\n",
    "    CodeConfiguration,\n",
    ")\n",
    "\n",
    "from azure.identity import DefaultAzureCredential, ManagedIdentityCredential"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 87,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 87,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "bert = models.Transformer('sentence-transformers/msmarco-distilbert-base-v4')\n",
    "\n",
    "pooling_model = models.Pooling(\n",
    "    bert.get_word_embedding_dimension(),\n",
    "    pooling_mode_mean_tokens=True\n",
    ")\n",
    "\n",
    "mymodel = SentenceTransformer(modules=[bert, pooling_model])\n",
    "mymodel"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 92,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.auto_model.embeddings.word_embeddings.weight\n",
      "0.auto_model.embeddings.position_embeddings.weight\n",
      "0.auto_model.embeddings.LayerNorm.weight\n",
      "0.auto_model.embeddings.LayerNorm.bias\n",
      "0.auto_model.transformer.layer.0.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.0.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.0.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.0.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.0.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.0.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.0.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.0.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.0.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.1.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.1.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.1.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.1.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.1.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.1.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.1.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.1.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.1.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.2.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.2.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.2.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.2.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.2.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.2.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.2.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.2.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.2.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.3.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.3.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.3.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.3.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.3.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.3.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.3.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.3.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.3.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.4.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.4.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.4.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.4.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.4.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.4.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.4.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.4.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.4.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.5.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.5.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.5.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.5.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.5.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.5.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.5.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.5.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.5.output_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in mymodel.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 93,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "0.auto_model.embeddings.word_embeddings.weight\n",
      "0.auto_model.embeddings.position_embeddings.weight\n",
      "0.auto_model.embeddings.LayerNorm.weight\n",
      "0.auto_model.embeddings.LayerNorm.bias\n",
      "0.auto_model.transformer.layer.0.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.0.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.0.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.0.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.0.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.0.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.0.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.0.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.0.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.0.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.0.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.1.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.1.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.1.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.1.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.1.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.1.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.1.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.1.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.1.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.1.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.1.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.2.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.2.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.2.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.2.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.2.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.2.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.2.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.2.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.2.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.2.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.2.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.3.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.3.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.3.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.3.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.3.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.3.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.3.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.3.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.3.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.3.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.3.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.4.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.4.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.4.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.4.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.4.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.4.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.4.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.4.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.4.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.4.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.4.output_layer_norm.bias\n",
      "0.auto_model.transformer.layer.5.attention.q_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.q_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.k_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.k_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.v_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.v_lin.bias\n",
      "0.auto_model.transformer.layer.5.attention.out_lin.weight\n",
      "0.auto_model.transformer.layer.5.attention.out_lin.bias\n",
      "0.auto_model.transformer.layer.5.sa_layer_norm.weight\n",
      "0.auto_model.transformer.layer.5.sa_layer_norm.bias\n",
      "0.auto_model.transformer.layer.5.ffn.lin1.weight\n",
      "0.auto_model.transformer.layer.5.ffn.lin1.bias\n",
      "0.auto_model.transformer.layer.5.ffn.lin2.weight\n",
      "0.auto_model.transformer.layer.5.ffn.lin2.bias\n",
      "0.auto_model.transformer.layer.5.output_layer_norm.weight\n",
      "0.auto_model.transformer.layer.5.output_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "for name, param in mymodel.named_parameters():\n",
    "    print(name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 117,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel ,\n",
       " Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False}))"
      ]
     },
     "execution_count": 117,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel[0], mymodel[1]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 124,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "auto_model.transformer.layer.5.attention.q_lin.weight\n",
      "auto_model.transformer.layer.5.attention.q_lin.bias\n",
      "auto_model.transformer.layer.5.attention.k_lin.weight\n",
      "auto_model.transformer.layer.5.attention.k_lin.bias\n",
      "auto_model.transformer.layer.5.attention.v_lin.weight\n",
      "auto_model.transformer.layer.5.attention.v_lin.bias\n",
      "auto_model.transformer.layer.5.attention.out_lin.weight\n",
      "auto_model.transformer.layer.5.attention.out_lin.bias\n",
      "auto_model.transformer.layer.5.sa_layer_norm.weight\n",
      "auto_model.transformer.layer.5.sa_layer_norm.bias\n",
      "auto_model.transformer.layer.5.ffn.lin1.weight\n",
      "auto_model.transformer.layer.5.ffn.lin1.bias\n",
      "auto_model.transformer.layer.5.ffn.lin2.weight\n",
      "auto_model.transformer.layer.5.ffn.lin2.bias\n",
      "auto_model.transformer.layer.5.output_layer_norm.weight\n",
      "auto_model.transformer.layer.5.output_layer_norm.bias\n"
     ]
    }
   ],
   "source": [
    "# Freeze all the parameters\n",
    "for name, param in mymodel[0].named_parameters():\n",
    "    param.requires_grad = False\n",
    "\n",
    "# Unfreeze the last few layers\n",
    "for name, param in mymodel[0].named_parameters():\n",
    "    if (name.__contains__('layer.5')):\n",
    "        print(name)\n",
    "        param.requires_grad = True\n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Class WorkspaceHubOperations: This is an experimental class, and may change at any time. Please see https://aka.ms/azuremlexperimental for more information.\n"
     ]
    }
   ],
   "source": [
    "subscription_id_ = \"10261ac3-459c-4a4d-bbb3-b4099e8d76b0\"\n",
    "resource_group_ = \"mlopsdev\"\n",
    "workspace_ = \"cgamlopssecondary\"\n",
    "\n",
    "\n",
    "ml_client = MLClient(\n",
    "    DefaultAzureCredential(), \n",
    "    subscription_id_, \n",
    "    resource_group_, \n",
    "    workspace_,\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "ws = Workspace.from_config()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{\n",
       "  \"name\": \"workspaceadlsstore\",\n",
       "  \"container_name\": \"models\",\n",
       "  \"account_name\": \"cgaadlsdev\",\n",
       "  \"protocol\": \"https\",\n",
       "  \"endpoint\": \"core.windows.net\"\n",
       "}"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ws.datastores['workspaceadlsstore']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "datastore = ws.datastores['workspaceadlsstore']\n",
    "\n",
    "datastore_paths = [(datastore, 'msx_copilot_content_recommendation/')]\n",
    "dataset = Dataset.File.from_files(path=datastore_paths)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "['ContentQuestions_FinalDataset.csv', 'conda_yaml', 'input', 'logs', 'models', 'output']\n",
      "/tmp/tmp9oieszua\n"
     ]
    }
   ],
   "source": [
    "mounted_path = tempfile.mkdtemp()\n",
    "\n",
    "# mount dataset onto the mounted_path of a Linux-based compute\n",
    "mount_context = dataset.mount(mounted_path)\n",
    "\n",
    "mount_context.start()\n",
    "\n",
    "import os\n",
    "print(os.listdir(mounted_path))\n",
    "print (mounted_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.read_csv(mounted_path + '/ContentQuestions_FinalDataset.csv',\n",
    "                                       low_memory=False)\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(20604, 5)"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [],
   "source": [
    "pd.set_option('display.max_colwidth', None)\n",
    "pd.set_option('display.max_rows',100)\n",
    "pd.set_option('display.max_columns',100)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "df['questions_cleaned']=df['question'].apply(lambda x: x[8:-2])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>content</th>\n",
       "      <th>question</th>\n",
       "      <th>label</th>\n",
       "      <th>counter</th>\n",
       "      <th>questions_cleaned</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0</td>\n",
       "      <td>006 Email Template: MCEM2_MTB_Comprehensive Threat Protection with XDR and SIEM_EN-NA_Boston_020724. Use this email template to generate interest with your EXISTING CUSTOMERS ONLY.  Email blast use is limited to existing customer relationships.. SMC Commercial, SMC Government, Major Public Sector, Strategic Commercial, Strategic Public Sector, SMC Education, Major Commercial. Security.</td>\n",
       "      <td>\"1\": \"What specific features of XDR and SIEM are highlighted in the MCEM2_MTB email template for enhancing threat protection?\",</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>What specific features of XDR and SIEM are highlighted in the MCEM2_MTB email template for enhancing threat protection?</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1</td>\n",
       "      <td>006 Email Template: MCEM2_MTB_Comprehensive Threat Protection with XDR and SIEM_EN-NA_Boston_020724. Use this email template to generate interest with your EXISTING CUSTOMERS ONLY.  Email blast use is limited to existing customer relationships.. SMC Commercial, SMC Government, Major Public Sector, Strategic Commercial, Strategic Public Sector, SMC Education, Major Commercial. Security.</td>\n",
       "      <td>\"2\": \"How does the MCEM2_MTB email template propose to integrate Comprehensive Threat Protection with existing systems in our organization?\",</td>\n",
       "      <td>1</td>\n",
       "      <td>1</td>\n",
       "      <td>How does the MCEM2_MTB email template propose to integrate Comprehensive Threat Protection with existing systems in our organization?</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0  \\\n",
       "0           0   \n",
       "1           1   \n",
       "\n",
       "                                                                                                                                                                                                                                                                                                                                                                                                content  \\\n",
       "0  006 Email Template: MCEM2_MTB_Comprehensive Threat Protection with XDR and SIEM_EN-NA_Boston_020724. Use this email template to generate interest with your EXISTING CUSTOMERS ONLY.  Email blast use is limited to existing customer relationships.. SMC Commercial, SMC Government, Major Public Sector, Strategic Commercial, Strategic Public Sector, SMC Education, Major Commercial. Security.   \n",
       "1  006 Email Template: MCEM2_MTB_Comprehensive Threat Protection with XDR and SIEM_EN-NA_Boston_020724. Use this email template to generate interest with your EXISTING CUSTOMERS ONLY.  Email blast use is limited to existing customer relationships.. SMC Commercial, SMC Government, Major Public Sector, Strategic Commercial, Strategic Public Sector, SMC Education, Major Commercial. Security.   \n",
       "\n",
       "                                                                                                                                          question  \\\n",
       "0                  \"1\": \"What specific features of XDR and SIEM are highlighted in the MCEM2_MTB email template for enhancing threat protection?\",   \n",
       "1    \"2\": \"How does the MCEM2_MTB email template propose to integrate Comprehensive Threat Protection with existing systems in our organization?\",   \n",
       "\n",
       "   label  counter  \\\n",
       "0      1        1   \n",
       "1      1        1   \n",
       "\n",
       "                                                                                                                       questions_cleaned  \n",
       "0                What specific features of XDR and SIEM are highlighted in the MCEM2_MTB email template for enhancing threat protection?  \n",
       "1  How does the MCEM2_MTB email template propose to integrate Comprehensive Threat Protection with existing systems in our organization?  "
      ]
     },
     "execution_count": 43,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.head(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "metadata": {},
   "outputs": [],
   "source": [
    "# df['question'][2789][8:-2]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'006 Email Template: MCEM2_MTB_Comprehensive Threat Protection with XDR and SIEM_EN-NA_Boston_020724. Use this email template to generate interest with your EXISTING CUSTOMERS ONLY.  Email blast use is limited to existing customer relationships.. SMC Commercial, SMC Government, Major Public Sector, Strategic Commercial, Strategic Public Sector, SMC Education, Major Commercial. Security.'"
      ]
     },
     "execution_count": 44,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df['content'][0]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = []\n",
    "for i in range(0,df.shape[0]):\n",
    "    samples.append(InputExample(\n",
    "        texts=[df['content'][i], df['questions_cleaned'][i]],\n",
    "        label=df['label'][i]\n",
    "    ))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "20604"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(samples)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [],
   "source": [
    "batch_size = 8"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 68,
   "metadata": {},
   "outputs": [],
   "source": [
    "loader = datasets.NoDuplicatesDataLoader(\n",
    "    samples, batch_size = batch_size\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 125,
   "metadata": {},
   "outputs": [],
   "source": [
    "loss = losses.ContrastiveLoss(mymodel)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 135,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "3"
      ]
     },
     "execution_count": 135,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "epochs = 2\n",
    "warmup = int(.0015*len(loader))\n",
    "warmup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 136,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "cpu\n"
     ]
    }
   ],
   "source": [
    "if torch.cuda.is_available():\n",
    "    device = torch.device(\"cuda\")\n",
    "    print(device)\n",
    "else:\n",
    "    device = \"cpu\"\n",
    "    print(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 137,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "SentenceTransformer(\n",
       "  (0): Transformer({'max_seq_length': 512, 'do_lower_case': False}) with Transformer model: DistilBertModel \n",
       "  (1): Pooling({'word_embedding_dimension': 768, 'pooling_mode_cls_token': False, 'pooling_mode_mean_tokens': True, 'pooling_mode_max_tokens': False, 'pooling_mode_mean_sqrt_len_tokens': False})\n",
       ")"
      ]
     },
     "execution_count": 137,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "mymodel.to(device)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 138,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'/home/azureuser/localfiles/MLOps/AML/msx_copilot_content_recomm'"
      ]
     },
     "execution_count": 138,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import os\n",
    "os.getcwd()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 139,
   "metadata": {},
   "outputs": [],
   "source": [
    "mymodel.fit(\n",
    "    train_objectives = [(loader, loss)],\n",
    "    epochs=epochs,\n",
    "    warmup_steps = warmup,\n",
    "    output_path='ms_marco_finetuned',\n",
    "    show_progress_bar=False\n",
    ")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "azureml_py38_PT_TF",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
